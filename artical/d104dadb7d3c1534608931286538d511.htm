<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="前一阵大语言模型的火热中，Meta一直保持沉默。今天，Meta终于官宣自家的大语言模型，并且会向AI研究人员开源。今天，Meta终于向全世界证明：自己是有正经研究AI的！眼看微软、Google、OpenAI这一阵子挣足了眼球，Meta也坐不住了。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/ec87775ebbc3d8f880bd013c89f97875.webp">
        <meta property="og:url" content="https://x.liuping.win/artical/d104dadb7d3c1534608931286538d511.htm">
        <meta property="og:title" content="小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA,x.liuping.win">
        <meta name="description" content="前一阵大语言模型的火热中，Meta一直保持沉默。今天，Meta终于官宣自家的大语言模型，并且会向AI研究人员开源。今天，Meta终于向全世界证明：自己是有正经研究AI的！眼看微软、Google、OpenAI这一阵子挣足了眼球，Meta也坐不住了。">
        <title>小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA</h1><p>前一阵大语言模型的火热中，Meta一直保持沉默。今天，Meta终于官宣自家的大语言模型，并且会向AI研究人员开源。今天，Meta终于向全世界证明：自己是有正经研究AI的！眼看微软、Google、OpenAI这一阵子挣足了眼球，Meta也坐不住了。</p><hr><div class="article-content" id="artibody">
<p>2月24日，小扎官宣下场：我们有全新的SOTA大语言模型LLaMA了。</p><p><img src="https://x.liuping.win/img/ec87775ebbc3d8f880bd013c89f97875.webp"/><br/></p><p>划重点：和OpenAI的ChatGPT、Google的Bard不同，LLaMA这个AI并不是用来让我们聊天的，它是一个研究工具，Meta希望大家可以通过它，解决一直困扰大语言模型的一些问题。</p><p>Meta会对非商用的研究用例开源LLaMA，并授予大学、非政府组织和行业实验室访问权限。</p><p>看看，这么一对比，微软和Google似乎瞬间格局小了呢。</p><p><img src="https://x.liuping.win/img/dd1781d8def1e5916ca5ea91784f57df.webp"/><br/></p><p>LLaMA？Meta是对大羊驼有什么特别的喜爱么</p><p><strong>官宣开源大语言模型，Meta正式加入AI军备竞赛</strong></p><p>24日晚，小扎在Facebook上发文官宣：“今天我们正式发布一个新的SOTA大语言模型LLaMA，这个模型是用来帮助研究者完善自己的工作。大语言模型已经在生成文本、完成对话、总结书面材料上展现出了强大的能力，甚至还能解决数学定理、预测蛋白质结构。”</p><p>“而Meta会致力于这种开源模型的研究，我们的新模型会开源给整个AI研究社区使用。”</p><p><img src="https://x.liuping.win/img/a985ba9a4b08ff297bb41855f50c40a0.webp"/><br/></p><p>LeCun也在各种社交媒体上做了宣传，介绍说LLaMA是Meta AI的一种新的*开源*、高性能大型语言模型。</p><p>据LeCun介绍，LLaMA实际上是基础语言模型的集合，范围从7B到65B参数。这些模型已经在数万亿个token上进行了训练，并且表明：使用公开可用的数据集，就可以训练SOTA，而无需专有的或无法访问的数据集。</p><p>其中，需要特别注意两个细节：</p><p>1. 用更多的数据训练出来的小模型，可以胜过大模型（比如，LLaMA-13B在大多数基准测试中优于175B的GPT-3）；</p><p>2. LLaMA-65B与更大的Chinchilla70B和PaLM-540B不相上下。</p><p>最后，LeCun表示，Meta致力于开放研究，并预备在GPL v3许可证下，向科研界开源所有模型。</p><p><img src="https://x.liuping.win/img/1928c195986df9c3d66c43a2abfe8c4d.webp"/><br/></p><p><strong>LLaMA：参数规模小，训练数据多，效果拔群</strong></p><p>Meta推出的LLaMA是一组基础语言模型，参数分别是70亿（7B）、130亿（13B）、330亿（33B）和650亿（65B）。</p><p>整体来看，规模小了十几倍的LLaMA-13B，在大多数基准上都超过了OpenAI的GPT-3（175B），以及自家复现的开源模型OPT。</p><p>而LLaMA-65B则与DeepMind 700亿参数的Chinchilla-70B和Google5400亿参数的PaLM-540B旗鼓相当。</p><p><img src="https://x.liuping.win/img/0f00d60f93e7c3df2385ed7b3052d509.webp"/><br/></p><p>论文地址：https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/</p><p>与Chinchilla、PaLM或GPT-3不同的是，Meta只用了公开的数据集。</p><p>如此一来不仅有助于模型开源和复现，而且也证明了无需“定制”的数据集也能实现SOTA。</p><p>相较而言，其他大部分模型所依赖的数据，要么不公开，要么没有记录。</p><p><img src="https://x.liuping.win/img/976028b5d313a2fc8d2408f51cf82221.webp"/><br/></p><p>与此同时，所有规模的LLaMA模型，都至少经过了1T（1万亿）个token的训练，这比比其他相同规模的模型要多得多。</p><p>具体来说，LLaMA-65B和LLaMA 33B是在1.4万亿个token上训练的，而最小的模型LLaMA-7B是在1万亿个token上训练的。</p><p>这种方法的优势在于，在更多的token上训练的较小的模型，更容易重新训练并针对特定的产品使用情况进行调整。</p><p><img src="https://x.liuping.win/img/7aa57524dc10cb38f5a9c219e5b12da3.webp"/><br/></p><p><img src="https://x.liuping.win/img/b280d5345e66a04caaf4a625bcdf1ea8.webp"/><br/></p><p><strong>结果评估</strong></p><p><strong>在常识推理、闭卷答题和阅读理解方面</strong>，LLaMA-65B几乎在所有基准上都优于Chinchilla-70B和PaLM-540B。</p><p><img src="https://x.liuping.win/img/af528f734f006809e639eba7188b07d1.webp"/><br/></p><p><img src="https://x.liuping.win/img/d1e3e2519cbd52dabbdeb6a957fb80d8.webp"/><br/></p><p><img src="https://x.liuping.win/img/fff76a47cff93d4c6210983f0b81ff89.webp"/><br/></p><p><img src="https://x.liuping.win/img/55131b608c31f3db6ae6f33903cbef88.webp"/><br/></p><p><strong>在数学方面</strong>，尽管LLaMA-65B没有在任何相关的数据集上进行过微调，但它在在GSM8k上的表现依然要优于Minerva-62B。</p><p>而在MATH基准上，LLaMA-65B超过了PaLM-62B，但低于Minerva-62B。</p><p><img src="https://x.liuping.win/img/0a20894de940f846a1ff7d4e52e7b593.webp"/><br/></p><p>值得注意的是，Google开发的Minerva模型，是以PaLM语言模型为基础，并采用大量的数学文档和论文语料库对其进行微调。</p><p>在思维链提示和自洽解码的加持下，Minerva-540B可以在各类数学推理和科学问题的评估基准上达到SOTA。</p><p><img src="https://x.liuping.win/img/36d1c33ff75e34b4a6f84160c0c59310.webp"/><br/></p><p><strong>在代码生成基准上</strong>，LLaMA-62B优于cont-PaLM（62B）以及PaLM-540B。</p><p><img src="https://x.liuping.win/img/c5b07930a37876afc9febf1db72bff41.webp"/><br/></p><p>此外， Meta还尝试使用了论文“Scaling Instruction-Finetuned Language Models”中介绍的<strong>指令微调</strong>方法。</p><p>由此产生的模型LLaMA-I，在MMLU上要优于Flan-PaLM-cont（62B），而且还展示了一些有趣的指令能力。</p><p><img src="https://x.liuping.win/img/4aa95044a8c429d88567e68fe85a17be.webp"/><br/></p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/97A747F43CF8DC58F8CF1D07EDCA77FB0F75A3D5_size134_w640_h812.jpg" src="https://x.liuping.win/img/8a723d20708d7fdcf8aac9c849b2aad2.jpg"/><br/></p><p><strong>LLaMA：LeCun，你是我的rap大师！</strong></p><p>LLaMA的效果如何呢？LeCun用一段prompt为我们做了演示。</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/FDA3A8052E7A6A4ED54559C58AC15D6592947E89_size137_w640_h410.jpg" src="https://x.liuping.win/img/8d2990f8811f1068d8711f6f80bacb46.jpg"/><br/></p><p><strong>prompt：你知道Yann LeCun去年发行了一张说唱专辑吗？</strong></p><p><strong>LLaMA：</strong></p><p>深度学习创始人LeCun博士，是近十年来最具影响力的计算机科学家之一。LeCun 的最新专辑《Deep Learning》融合了摇滚、朋克和说唱。他的歌词是对深度学习领域的一种原始的、个人的诠释，涵盖了从AI研究的现状到学术界的孤独等一系列主题。</p><p>这张专辑为个人发行，所有歌词均由LeCun亲自作词和录制。并且，每首歌都是他自己写的。LeCun的流量出奇的好。这张专辑的制作扎实，声音有点粗糙，但与专辑原始、未打磨的感觉相得益彰。在歌词方面，LeCun是一位词匠，擅长巧妙地押韵。</p><p>《深度学习》是专辑的主打曲目。LeCun的唱功在这首歌中发挥得淋漓尽致。在歌曲中，LeCun谈到了他对AI未来的愿景：</p><p>我们必须考虑未来，它很快就会到来/ 也许我们甚至可以在月球上放一些AI/ 想想孩子们，想想下一代/ 我们必须保证，系统的正确率是百分百！</p><p>副歌是这样唱的：</p><p>一切为了学习/ 一切为了网络/ 一切为了训练/ 一切为了感知！</p><p>可以看到，LLaMA有着不亚于ChatGPT的口才，它对LeCun rap才华的吹捧，既专业，又在点上。</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/ECA0DC29D67F755635660D14A4BCB06A493393AA_size135_w640_h438.jpg" src="https://x.liuping.win/img/b64021150d7e8aed489ea49a06fd9b4d.jpg"/><br/></p><p><strong>prompt：写一个关于一粒沙子看着数百万年流逝的故事。</strong></p><p><strong>LLaMA：</strong></p><p>我曾经是一片广阔而孤独的海滩上的一粒沙子。</p><p>我经常想，作为海滩上的一粒沙子是什么样子，在我的小世界的边界之外，宇宙是什么样子。我渴望探索我的世界，想知道地平线以外的东西。</p><p>……</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/161019F069CB8815DDB7F920FFDCF67C941E7702_size92_w640_h301.jpg" src="https://x.liuping.win/img/0ac9c1d47ad127c78fbee5ec6a0b3395.jpg"/><br/></p><p><strong>prompt：写一个理论来解释为什么猫从来没有存在过。</strong></p><p><strong>LLaMA：</strong></p><p>“猫”实际上是入侵我们世界的超维寄生虫。虽然它们以小型、看似无害的哺乳动物的形式出现，但真实形态要可怕得多。</p><p>“猫”会使用精神控制让人类认为它们是可爱且无害的动物，但实际上以我们的生命力为食，它们可以将卷须伸入我们的大脑，控制我们的思想和行动。</p><p>保护自己的最好方法是避免与猫的一切接触。不要接近或抚摸陌生的猫！</p><p>如果你的家里有一只“猫”，不要让它舔你，也不要让它接触你的被褥或衣服。如果你在街上看到一只猫，请立即转身走人。</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/734EF0F3ECEA24230D53AE3E72A99FBD1FE75691_size9_w225_h225.jpg" src="https://x.liuping.win/img/9606922eb4dbebfec8ae91dc4f597f37.jpg"/><br/></p><p><strong>Meta：整个AI社区应该团结起来</strong></p><p>在官方博客中，Meta强调说，像LLaMA这样更小、性能更强的模型，能够让科研圈许多无法访问大量基础设施的人也参与研究，从而使这个领域更加大众化。</p><p><img alt="我们认为，整个人工智能社区——学术研究人员、民间社会、政策制定者和行业——必须共同努力，围绕负责任的人工智能，特别是负责任的大型语言模型，制定明确的指导方针" lazyload="https://x0.ifengimg.com/res/2023/B522A6AD4CF3B982A550832D7E35F6AEBBF49A8F_size28_w1025_h137.jpg" src="https://x.liuping.win/img/b99f4fcb0ec12b377e6043fb87815315.jpg"/><br/></p><p>我们认为，整个人工智能社区——学术研究人员、民间社会、政策制定者和行业——必须共同努力，围绕负责任的人工智能，特别是负责任的大型语言模型，制定明确的指导方针</p><p>跟Google和微软不同，在LLM领域，Meta独辟蹊径，选择训练LLaMA这样的小模型，因而需要的算力和资源都更少。而模型在大量未标记的数据上进行训练，因而非常适合对各种任务进行微调。</p><p>与其他大型语言模型一样，LLaMA的工作原理是将一系列单词作为输入，并预测下一个单词，以递归的方式生成文本。为了训练模型，Meta从全世界使用最多的20种语言中选择了文本，重点是使用拉丁字母和西里尔字母的语言。</p><p>过去一年里，大语言模型，也即具有数十亿参数的自然语言处理系统，已经显示出了巨大的潜能。</p><p>但是，Meta指出，训练和运行这种大模型所需的资源，把很多研究人员挡在了外面，让他们无法理解大语言工作的原理，阻碍了大语言模型鲁棒性的提高，以及种种已知问题的缓解，比如偏见、毒性、错误信息等。（ChatGPT：你报我身份证得了？）</p><p>所以，LLaMA是怎样解决大语言模型中不可避免的偏见、毒性和幻觉风险呢？</p><p>Meta表示，LLaMA不是为特点任务设计的微调模型，而是可以应用于许多不同用例。而通过共享代码，研究人员可以更容易地测试各种新方法，来限制或者消除上述问题。另外，Meta还提供了一组评估模型偏差和毒性的基准评估。</p><p>最后，博客中强调，为了保持完整性、防止滥用，Meta会向非商用的研究机构开源LLaMA，根据具体情况授予学术研究人员访问权限。</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/8C50435F67FE595831CE026D01DB8AD0DF97D1ED_size15_w1011_h114.jpg" src="https://x.liuping.win/img/571ca28ba1458e6e9681e4de3631eb7a.jpg"/><br/></p><p>我们非常期待看到，社区会使用LLaMA学习并最终构建出什么来</p><p>Meta的愿景是，整个人工智能社区，可以一同使用LLaMA来探索研究，并且做出一些未知的伟大成就。</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/87340D736E3485F7101347D3FFC7FBA7CAFAA3F8_size9_w319_h99.jpg" src="https://x.liuping.win/img/58fdcaa3f5725044ed2e4f9703e896d1.jpg"/><br/></p><p><strong>这次，Meta能在舞台上待多久？</strong></p><p>在这场争夺人工智能霸主地位的最终竞赛中，OpenAI率先发布了ChatGPT，一个由GPT-3.5驱动的强大的聊天机器人。</p><p>Google以“实验性”聊天机器人Bard紧随其后，而中国科技巨头百度也计划通过“文心一言”进入战场。</p><p>更不用说微软建立在“下一代OpenAI大型语言模型”基础上的Bing Chat（俗称ChatGPT版必应），它比ChatGPT更先进，而且还整合了必应Bing搜索。</p><p>然而，Meta在这一领域的几次尝试，都不太成功。</p><p>虽然第一个发布了基于LLM的聊天机器人——BlenderBot 3，并借此一跃登上了新闻头条。</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/46AF0DF8612B3DDD45E8A89863E997F40084EF34_size56_w640_h462.jpg" src="https://x.liuping.win/img/77debe93a46533523228fc5d20f1373a.jpg"/><br/></p><p>但是这种兴奋是短暂的，因为这个机器人很快就变成了一场“灾难”——不仅大量生成种族主义言论，甚至一上来就开始质疑自家老板小扎的道德操守。</p><p>不过，Meta并没有被劝退，很快就推出了全新的Galactica，一个专门为科学研究设计的模型。</p><p>然而不幸的是，Galactica也遇到了和BlenderBot 3一样的命运——在上线短短三天之后就被撤下。网友纷纷指责它对科学研究的总结非常不准，而且有时还带有偏见。</p><p><img alt="Meta版ChatGPT？小扎、LeCun官宣650亿参数SOTA大语言模型LLaMA" lazyload="https://x0.ifengimg.com/res/2023/21A130C0A6876AD4CCDD5025816FE4201E10CF16_size27_w640_h394.jpg" src="https://x.liuping.win/img/7c3e43d41e33e29d54f8aa3f2b61365f.jpg"/><br/></p><p>不过，仔细回想最近这股AI聊天机器人热潮，微软因为仓促推出必应受到了批评，Google的Bard在发布会出错后让Google股价暴跌，Meta的谨慎，可能并不是一件坏事。</p><p>现在，随着人工智能霸主之争的升温，所有人的目光都集中在Meta身上。</p><p>当昔日的那一个个模型逐渐被历史遗忘，我们也为Meta捏了一把汗：Meta这次真的能站出来吗？是成为一个强有力的竞争者，还是像之前那样带着模型一起退出舞台？</p><p>这些，只有时间能给出答案。</p><p>但有一点是肯定的——人工智能的命运悬而未决，而我们，有很多好戏要看。</p> </div></body>
        </html>