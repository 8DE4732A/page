<!DOCTYPE html>
            <html><head>
            <meta name="viewport" content="width=device-width,initial-scale=1">
            <meta charset="utf-8">
            <meta property="og:type" content="article">
            <meta property="og:locale" content="zh_CN">
            <meta property="og:description" content="罗马不是一天建成的。当人工智能对话工具ChatGPT一夜之间成为顶流，在略显沉闷的科技界如闪电般发出炫目光芒后，它似乎点亮了指引投资界方向的明灯，一些商界人士的内心开始“骚动”。">
            <meta property="og:site_name" content="x.liuping.win">
            <meta property="og:image" content="https://x.liuping.win/img/7ed060fb9b9c6ee88532f93a71a9c3e5.webp">
            <meta property="og:url" content="https://x.liuping.win/artical/61e1cb4d0db92355b291343757d42b54.htm">
            <meta property="og:title" content="ChatGPT不是一天建成的：人类如何用66年实现今天的AI聊天？ - x.liuping.win">
            <meta name="keywords" content="AI 人工智能,ChatGPT不是一天建成的：人类如何用66年实现今天的AI聊天？,x.liuping.win">
            <meta name="description" content="罗马不是一天建成的。当人工智能对话工具ChatGPT一夜之间成为顶流，在略显沉闷的科技界如闪电般发出炫目光芒后，它似乎点亮了指引投资界方向的明灯，一些商界人士的内心开始“骚动”。">
            <title>ChatGPT不是一天建成的：人类如何用66年实现今天的AI聊天？</title>
            <style>img {max-width: 90%;} body {text-align: center;}</style>
            </head>
            <body><h1>ChatGPT不是一天建成的：人类如何用66年实现今天的AI聊天？</h1><p>罗马不是一天建成的。当人工智能对话工具ChatGPT一夜之间成为顶流，在略显沉闷的科技界如闪电般发出炫目光芒后，它似乎点亮了指引投资界方向的明灯，一些商界人士的内心开始“骚动”。</p><hr><div class="article-content" id="artibody">
<p>的确，这个成绩是史无前例的。ChatGPT是有史以来用户增长最快的互联网服务，推出仅两个月就获得了1亿用户。它被内置于微软的必应搜索引擎中，把Google顷刻间拉下神坛，正在促成搜索引擎自诞生以来的重大转折点。</p><p>但ChatGPT绝非凭空而来。这款聊天机器人是多年来一系列大型语言模型中最完善的一个。梳理ChatGPT的简要历史就会发现，在其诞生前，有无数技术的迭代、理论的发展为它铺路。</p><p><strong>20世纪五六十年代：符号与亚符号人工智能</strong></p><p>人工智能这一术语始于1956年的美国达特茅斯学院，经历几十年“三起两落”的发展阶段，有过“寒冬”，也有过“盛夏”：几次重大事件让一度归于沉寂的人工智能研究再次成为被广泛讨论的热门话题。ChatGPT的成功，源于以深度学习为代表的人工智能技术的长期积累。</p><p><img src="https://x.liuping.win/img/7ed060fb9b9c6ee88532f93a71a9c3e5.webp"/><br/></p><p>1956年达特茅斯会议，约翰·麦卡锡、马文·明斯基、克劳德·香农、艾伦·纽厄尔、赫伯特·西蒙等科学家正聚在一起，讨论用机器来模仿人类学习以及其他方面的智能。这一年被誉为人工智能诞生元年。</p><p>人工智能领域主要有两类，一类是符号人工智能（symbolic AI），另一类是以感知机为雏形的亚符号人工智能（subsymbolic AI）。前者的基本假设是智能问题可以归为“符号推理”过程，这一理论可追溯至计算机鼻祖、法国科学家帕斯卡和德国数学家莱布尼茨，真正体现这一思想的所谓智能机器，源于英国的查尔斯·巴贝奇（Charles Babbage）和艾伦·图灵（Alan Turing）的开创性工作。</p><p>亚符号人工智能的出现归功于行为主义认知理论的崛起，其思想基础是“刺激-反应理论”。美国神经生理学家沃伦·麦克卡洛克（Warren McCulloch）、沃尔特·皮茨（Walter Pitts）提出神经元模型后，心理学家弗兰克·罗森布拉特（Frank Rosenblatt）提出了感知机模型，奠定了神经网络的基础。</p><p>早期的神经网络技术秉承人工智能深度学习“逐层递进、层层抽象”的基本思想，出现了诸如MCP神经元、感知机和前馈神经网络等模型，它们通常由多个处理信息且相互连接的“神经元”组成，其灵感来自人脑中所连接神经元之间的信息交换。</p><p>20世纪五六十年代，人工智能在符号演算和感知机两个方向上都陷入了停滞。在麻省理工学院和加州大学伯克利分校任教的休伯特·德雷福斯（Hubert Dreyfus ）1965年发表《炼金术与人工智能》报告，将当时所进行的神经网络研究与历史上的炼金术相提并论，辛辣指出爬上树梢不等于攀登月球。1973年，“莱特希尔报告”对当时的符号主义人工智能提出批评，认为“迄今的发现尚未产生当时承诺的重大影响”，人工智能第一次跌入低谷。</p><p>80年代兴起的专家系统和神经网络，也因为受制于计算能力和对智能的理解，并未获得实质性的突破，使得人工智能跌入了第二次低谷。</p><p>但从80年代开始，一棵大树已经播种。</p><p><strong>20世纪八九十年代：递归神经网络</strong></p><p>理解和使用自然语言是人工智能面临的最大挑战之一。语言常常充满歧义，极度依赖语境，而且通常用语言沟通的各方需要具备大量共同的背景知识。与人工智能的其他领域一样，自然语言处理相关的研究在最初的几十年集中在符号化的、基于规则的方法上，并没有取得很好的效果。递归神经网络（Recurrent Neural Networks）改变了一切。</p><p>ChatGPT是基于大型语言模型GPT-3的一个对话式版本，而语言模型是一种经过大量文本训练的神经网络。由于文本是通过不同长度的字母和单词序列组成，语言模型需要一种能够“理解”这类数据的神经网络，发明于20世纪80年代的递归神经网络可以处理单词序列。但有一个问题是，它们的训练速度很慢，而且可能会忘记序列中之前的单词。</p><p>1997年，计算机科学家斯皮·哈切瑞特（Sepp Hochreiter）和尤尔根·斯成杜博（Jürgen Schmidhuber）通过发明长短期记忆（LSTM）网络解决了这个问题，这是一种具有特殊成分的循环神经网络，可以让输入序列中的过去的数据保留更长时间。LSTMs可以处理几百个单词长的文本字符串，但他们的语言技能有限。</p><p>在人工智能处理自然语言出现重大突破前夕，神经网络和机器学习在2016年发生了一次“出圈”事件。Google公司的AlphaGo在各种围棋比赛中大获全胜，给全世界做了一次人工智能科普。DeepMind创始人之一沙恩·莱格（Shane Legg）认为，超越人类水平的人工智能将在2025年左右出现。Google公司战略委员会成员雷·库兹韦尔（Ray Kurzweil）则提出了令人震惊的“奇点理论”，认为2029年完全通过图灵测试的智能机器将会出现，以强人工智能为基础的智能爆炸将会在2045年出现。</p><p><img src="https://x.liuping.win/img/7ca02f243c9dfa9e161c3007e62953f7.webp"/><br/></p><p>AlphaGo战胜李世石和柯洁。</p><p><strong>2017年：Transformer</strong></p><p>Google的一个研究团队发明了Transformer，这是一种神经网络，可以跟踪每个单词或短语在序列中出现的位置，从而实现了当今这一代大型语言模型背后的突破。单词的含义通常取决于前面或后面其他单词的含义。通过跟踪这些上下文信息，Transformer可以处理更长的文本字符串，并更准确地捕捉单词的含义。例如，“hot dog”在“Hot dogs should be given plenty of water（狗热了要多喝水）”和“Hot dogs should be eaten with mustard（热狗应该和芥末酱一起吃）”这两个句子中的含义截然不同。</p><p><img src="https://x.liuping.win/img/ebc84eafbd48c2e3076c2f2b1df5f52f.webp"/><br/></p><p>Google发布Transformer的那篇著名论文。</p><p>Transformer能够同时并行进行数据计算和模型训练，训练时长更短，并且训练得出的模型可用语法解释，也就是模型具有可解释性。</p><p>经过训练后，Transformer在包括翻译准确度、英语成分句法分析等各项评分上都达到了业内第一，成为当时最先进的深度学习模型。</p><p>Transformer自诞生的那一刻起，就深刻地影响了接下来几年人工智能领域的发展轨迹。短短的几年里，该模型的影响已经遍布人工智能的各个领域——从各种各样的自然语言模型到预测蛋白质结构的AlphaFold2模型，用的都是它。</p><p><strong>2018年：GPT-1</strong></p><p>在Transformer诞生还不到一年的时候，人工智能研究机构OpenAI推出了具有1.17亿个参数的GPT-1模型，GPT是Generative Pre-training Transformer（生成式预训练Transformer）的缩写，即用大量数据训练的基于Transformer的模型。该公司希望开发多技能、通用的人工智能，并相信大型语言模型是实现这一目标的关键一步。</p><p>GPT将Transformer与无监督学习相结合，这是一种根据事先未注释的数据训练机器学习模型的方法。这让软件可以自己找出数据中的模式，而无需被告知它在看什么。机器学习先前的许多成功都依赖于监督学习和注释数据，但手动标记数据是一项缓慢的工作，因此限制了可用于训练的数据集的大小。</p><p>GPT最终训练所得的模型在问答、文本相似性评估、语义蕴含判定以及文本分类这四种语言场景，都取得了比基础Transformer模型更优的结果，成为了新的业内第一。</p><p><img src="https://x.liuping.win/img/3c877ac08f4c1356fe614673e0d41fcc.webp"/><br/></p><p>为了创造通用人工智能，“你需要有数十亿美元的投资。”OpenAI LP的首席科学家Ilya Sutskever（左）2019年说。他与该公司当时的首席技术官Greg Brockman坐在一起。</p><p><strong>2019年：GPT-2</strong></p><p>微软向OpenAI投资了十亿美元。同年，OpenAI公布了具有15亿个参数的模型：GPT-2。该模型架构与GPT-1原理相同，主要区别是GPT-2的规模更大（10倍）。同时，他们发表了介绍这个模型的论文“Language Models are Unsupervised Multitask Learners” （语言模型是无监督的多任务学习者）。在这项工作中，他们使用了自己收集的以网页文字信息为主的新的数据集。不出意料，GPT-2模型刷新了大型语言模型在多项语言场景的评分纪录，引起了更大的轰动。但OpenAI称，他们非常担心人们会使用GPT-2“产生欺骗性的、有偏见的或辱骂性的语言”，因此不会发布完整的模型。</p><p><strong>2020年：GPT-3</strong></p><p>GPT-2令人印象深刻，但OpenAI的后续GPT-3引起了更大的反响，它实现了生成类人文本能力的巨大飞跃。GPT-3可以回答问题、总结文档、生成不同风格的故事，在英语、法语、西班牙语和日语之间进行翻译等。它的模仿能力不可思议。</p><p>最显著的收获之一是，GPT-3的收益来自于现有技术的超大规模化，而不是发明新技术。 GPT-3有1750亿个参数，比前两款GPT模型要大得多：经过基础过滤的全网页爬虫数据集（4290亿个词符）、维基百科文章（30亿词符）、两个不同的书籍数据集（一共670亿词符）。它的模型架构与GPT-2没有本质区别。</p><p>GPT-3面世时未提供广泛的用户交互界面，并且要求用户提交申请，申请批准后才能注册，所以直接体验过GPT-3模型的人并不多。</p><p>早期测试结束后，OpenAI对GPT-3进行了商业化：付费用户可以通过应用程序接口（API）连上GPT-3，使用该模型完成所需语言任务。2020年9月，微软公司获得了GPT-3模型的独占许可，意味着微软可以独家接触到GPT-3的源代码。</p><p>与此同时，上一代的缺点进一步被放大，Google的人工智能伦理团队联合主管蒂姆尼特·格布鲁（Timnit Gebru）与人合著了一篇论文，强调了与大型语言模型相关的潜在危害，但该论文不受公司内部高级经理的欢迎。2020年12月，格布鲁被解雇。</p><p><strong>2022年1月：InstructGPT</strong></p><p>GPT-3公测期间用户提供了大量的对话和提示语数据，而OpenAI内部的数据标记团队也生成了不少人工标记数据集。OpenAI用这些数据对GPT-3用监督式训练进行了微调，并收集了微调过的模型生成的答案样本，使用奖励模型和更多的标注过的数据继续优化微调过的语言模型，并且进行迭代，最终得到了InstructGPT。InstructGPT更善于遵循人的指示，并且产生更少的冒犯性语言、更少的错误信息和更少的整体错误。</p><p>大型语言模型一个普遍的问题是，训练它们的成本，使得只有最富有的实验室才能创建一个。这引发了人们的担忧，即这种强大的人工智能是由小型企业团队秘密开发的，没有经过适当的审查，也没有更广泛的研究社区的投入。作为回应，一些合作项目开发了大型语言模型，并将它们免费发布给任何想要研究和改进该技术的研究人员。Meta构建并给出了OPT，这是GPT-3的重构。Hugging Face领导了一个由大约1000名志愿研究人员组成的联盟来构建和发布BLOOM。</p><p><img src="https://x.liuping.win/img/c68ba7f963bd97032f12f838309ab8d7.webp"/><br/></p><p>OpenAI工作人员和Dota 2电子竞技团队OG的成员一起拍照。</p><p><strong>2022年12月：ChatGPT</strong></p><p>最终，2022年12月，ChatGPT面世。与InstructGPT模型类似，ChatGPT是OpenAI对GPT-3模型微调后开发出来的对话机器人。OpenAI官网信息显示，ChatGPT与InstructGPT是姐妹模型。与InstructGPT一样，ChatGPT使用强化学习对人类测试人员的反馈进行了训练，这些测试人员对其表现进行了评分，使其成为流畅、准确且无害的对话者。从此以后，全球有1亿人在和它聊天。</p><p>用户们在社交媒体上晒出来的对话例子表明，ChatGPT能完成包括写代码、代码改错、翻译文献、写小说、写商业文案、创作菜谱、做作业、评价作业等一系列常见文字输出型任务。ChatGPT比GPT-3更优秀的一点在于，前者在回答时更像是在与用户对话，而后者更善于产出长文章，欠缺口语化的表达。</p><p>ChatGPT一夜走红之后，在全球引发了高度关注，有业内人士认为它将影响包括搜索引擎、广告业、教育行业等领域。2022年12月，Google内部发布红色警报，着手进行紧急应对。</p><p>在接受《时代》专访时，ChatGPT回答道：我还有很多局限，但人类应准备好应对AI。</p> </div></body>
            </html>