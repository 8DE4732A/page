<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="眼看AI起高楼、眼看它楼塌了？就在昨天早上，一封公开信刷屏了朋友圈，著名安全机构生命未来研究所（&nbsp;FutureofLifeInstitute&nbsp;，&nbsp;FLI&nbsp;）呼吁全球所有研究机构暂停训练比GPT-4更强的AI。至少六个月。并且要在这六个月中来制订和AI相关的安全协议，其中就包括如何监管AI输出的内容，还有如何把AI创造出的内容和真实的内容区分开来。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/ea4326d2d32ff17040397ed031e894ae.png">
        <meta property="og:url" content="https://x.liuping.win/artical/ca596ff7d99deb370409e68cd361162d.htm">
        <meta property="og:title" content="几百名大佬联名给ChatGPT们踩刹车：AI到了失控边缘？ - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,几百名大佬联名给ChatGPT们踩刹车：AI到了失控边缘？,x.liuping.win">
        <meta name="description" content="眼看AI起高楼、眼看它楼塌了？就在昨天早上，一封公开信刷屏了朋友圈，著名安全机构生命未来研究所（&amp;nbsp;FutureofLifeInstitute&amp;nbsp;，&amp;nbsp;FLI&amp;nbsp;）呼吁全球所有研究机构暂停训练比GPT-4更强的AI。至少六个月。并且要在这六个月中来制订和AI相关的安全协议，其中就包括如何监管AI输出的内容，还有如何把AI创造出的内容和真实的内容区分开来。">
        <title>几百名大佬联名给ChatGPT们踩刹车：AI到了失控边缘？</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>几百名大佬联名给ChatGPT们踩刹车：AI到了失控边缘？</h1><p>眼看 AI 起高楼、眼看它楼塌了？就在昨天早上，一封公开信刷屏了朋友圈，<strong>著名安全机构生命未来研究所（ Future of Life Institute ， FLI ）呼吁全球所有研究机构暂停训练比 GPT-4 更强的 AI。至少六个月。</strong>并且要在这六个月中来制订和 AI 相关的安全协议，其中就包括如何监管 AI 输出的内容，还有如何把 AI 创造出的内容和真实的内容区分开来。</p><hr><div class="article-content" id="artibody">
<p><img src="https://x.liuping.win/img/ea4326d2d32ff17040397ed031e894ae.png"/></p><p>咱别看这个研究所名字中二感满满，但在上面留下名字的，个个都是行业里的顶尖大牛。</p><p><strong>图灵奖得主约书亚 · 本吉奥、苹果联合创始人史蒂夫 · 沃兹尼亚克、《 人类简史 》的作者尤瓦尔 · 赫拉利、以及什么地方都会刷一脚存在感的埃隆马斯克。</strong></p><p>可以说是 “ 满级人类闪耀时 ” 了。</p><p>虽然，关于这次签名事件的准确性没有那么高，也有些人在自己不之情的情况下 ”被签名“ 了，比如一开始 OpenAI CEO 奥特曼的签名也在上面（ 我杀我自己？ ）</p><p>再比上面还有急速追杀的主角 John Wick</p><p><img src="https://x.liuping.win/img/5b9af5d7e5da4e7f8d9ec57efb5394ce.png"/></p><p>但不管怎么说，可以感到大家都紧张起来了。</p><p>在信中，生命未来研究所提了四个问题：</p><p><strong>我们一定要让机器用宣传和谎言充斥我们的信息渠道吗？</strong></p><p><strong>我们一定要把所有工作都自动化吗？包括哪些人工完全可以做得令人满意的工作吗？</strong></p><p><strong>我们一定要发展最终可能超过我们、超越我们、并取代我们的非人类思维吗？</strong></p><p><strong>我们一定要冒险失去对我们文明的控制吗？</strong></p><p>讲道理，这些问题在过去几十年里一直是科幻小说中流行的题材。</p><p>但是可能谁也想不到，随着去年年底 OpenAI 一声枪响。这些难题真真切切的被搬到了我们面前，变成了我们可能需要立刻面对的元素。</p><p><strong>&gt;/ 眼见为真？耳听为实?</strong></p><p>前几天，有人在网上发布了一组图片，记录了 2001 年发生在美国西部小镇卡斯卡迪亚上的了一场 9.1 级的地震之后的景象。</p><p><img src="https://x.liuping.win/img/da0b4ba9c730de75a7e161f9942d9438.png"/></p><p>略带模糊的镜头记录下了当时哭喊的民众</p><p><img src="https://x.liuping.win/img/7ecb18614200002625a8792d65b16ce7.png"/></p><p>被摧毁的建筑和房屋</p><p><img src="https://x.liuping.win/img/683e61d8beada0e1353016b26a258d1e.png"/></p><p>甚至还有当时的美国总统小布什访问灾区的合影</p><p>无论是破损的城市，街道上大家的穿着，都在告诉观众这是一组来自 2001 年的照片。</p><p>不那么清晰的画质反而给大家一种 “ 路人随手拍 ” 的感觉，让照片的真实性再上了一个台阶。</p><p><strong>但唯一的问题是，根本没有这场大地震，卡斯卡迪亚上一次有记录的灾难，是在 1700 年。</strong></p><p><img src="https://x.liuping.win/img/4f5d23e3e04db058e63ca916fdf22e98.png"/></p><p>没错，这组照片就是用 Midjounrney 来生成的。</p><p>比起前段时间流行的 “ 赛博 COS” ，明暗对比更强烈，更擅长描述场景关系的 Midjourney 用来制作 “ 虚假历史 ” 明显更加得心应手了。</p><p>也更容易骗到大家的眼睛。前几天流行的 “ 川普被捕 ” 或者是 “ 教皇穿羽绒服 ” ，都是出于 Midjourney 的手笔。</p><p><img src="https://x.liuping.win/img/97e7771346a997d984659b9cf2cbfdb7.png"/></p><p>图片来源 @ 谷大白话</p><p>说实话，看到这个照片的第一瞬间，还真觉得有点懵，毕竟做为解放神学出身的教皇，方济各穿个羽绒服也很正常对吧。</p><p>但是，这些图片用来图一乐倒是还好，一旦用来深究，那事情可能就大条了。</p><p>万一下回，这项技术被用来诈骗呢？</p><p><strong>和前些年流行过的 DeepFake 换脸不同，如今的 AI 作图不需要特别好的显卡，更不需要很长的时间去调试，对 Midjourney 来说，只要坐在浏览器前敲敲键盘就行。</strong></p><p>当 AI 让图像造假的成本足够低廉之后，咱又要花多少时间去分辨一张图是不是 AI 画的？</p><p>比如川普返乡：</p><p><img src="https://x.liuping.win/img/e85cfb7a75a66358a03809e2bc8e88a8.png"/></p><p>可能生成一张假的图片需要 10 秒，但咱们想要看出来是这张图是不是 AI 画的，可能少说得要花个半分钟。</p><p><strong>不是每个人都有那么多精力能分辨 AI 的，只要有 1% 的人认不出来上当受骗，就可以说是一本万利的生意了好吧。</strong></p><p>以往我们说耳听为虚，眼见为实，然而现在，眼见也不再为真。</p><p>过去几十年来人类习以为常的秘密，已经悄悄被 AI 解构成了一堆的线性矩阵，在排列组合中，对人类瞒天过海。</p><p>而图片生成还只是 AIGC （ 人工智能技术来生成内容 ）中的一部分。</p><p>这次被大佬们试图联名叫停的 “ 巨型 AI” 更是重量级，对标的产品自然是出尽风头的 ChatGPT 。</p><p><img src="https://x.liuping.win/img/9aac625303c5141543b8a9c20f3fb435.png"/></p><p>这玩意有多离谱，咱们也写文章聊过很多，今天就不再做赘述。</p><p>前段时间还生成了一段杭州取消限行的文案，骗过了不少人。</p><p><img src="https://x.liuping.win/img/435a823234d1a491a479ae2e9efd8a09.png"/></p><p><strong>实际上，这类大语言模型并不是没有监管，从它们出生的那刻开始，监管就如影随形了。</strong></p><p><strong>上个月 OpenAI 在发布 GPT-4 的时候就提到，他们早在 22 年 8 月就完成了 GPT-4 的开发，但是之后花了六个月来搞安全问题，为的就是不让AI  “ 乱说话 ” 。</strong></p><p>GPT-4 经过拷打之后胡言乱语的水平大幅度下降：</p><p><img src="https://x.liuping.win/img/972818050244d2c21c1933168d8f63c6.png"/></p><p>但问题是，<strong>现在的 AI 和过去那些一步步执行编译的程序不一样，这 AI 有一个算一个，全是黑箱。 </strong></p><p>不像过去那些程序，要是出了问题，程序员还可以进去打断点，一个环节一个环节的调试 Bug 来把问题给搞清楚，哪一个环节有问题，针对性的改一下就好了。</p><p><strong>但现在以 Chatgpt 为首的大模型 AI 就好像一个黑色的纸箱子，你只知道给它喂苹果，它能还你一个橘子，而中间发生了什么事情一概不知。</strong></p><p>那万一这箱子要是出了问题，那咋办嘛？只能重新买一个黑箱子了。</p><p>ChatGPT 就像一盒巧克力，在它输出完成之前，没有人知道它嘴巴里会蹦出什么字来。</p><p>李彦宏也提到： “ 生成式 AI 每次给出的答案不一定一样，会带来不确定性。 ”</p><p><img src="https://x.liuping.win/img/b2831624427b6606b4071e1b0ebfd58e.png"/></p><p>这个生成到底有多不靠谱呢？</p><p>讲个刚刚遇到的事儿：前几天不是索尼微软任天堂、育碧和 D 社都说要退出 E3 展会么，我就有点好奇 E3 展会租金到底要多少钱？</p><p>先是在知乎上找到了一个回答，答主表示自己查到在 06 年时 E3 的租金就要 500万美元以上。</p><p>额……06 年，500万？这个数据好像有一些不靠谱，于是我又去用 NewBing 问了一下：也给出了类似的回答。</p><p><img src="https://x.liuping.win/img/2bdd1da0e047a306fe11bc50291a105d.jpg"/></p><p>有数据有链接，看起来是有理有据。</p><p>但问题是当我点开这个链接的时候发现……IGN 的文章里面根本没有提到租金 500 万这个数字。</p><p><img src="https://x.liuping.win/img/0e3e3e51f80f5d2665adb01da4e90310.jpg"/></p><p><strong>合着你是直接把之前的知乎文章翻译了一遍，然后又给我编了一篇英文文章来讹我啊。</strong></p><p>它顺应了我们的猜测给了一个谣言，要不是我多心看了一眼，可能真的给他糊弄过去了。</p><p>真就是睁眼说瞎话大师。</p><p>除了这个 title 以外， AI 还是桀骜不羁的漏洞寻找大师，往往能在我们训练它的过程中表演一手出乎意料。</p><p><strong>在人工智能领域，有个非常经典的理论叫做 “ 对齐问题 ” ，意思是 AI 在做的事情和我们想要的结果之间没有对齐，命令的传递出现了偏差。</strong></p><p>就比如前两年那只宁愿一头撞死的赛博狼，发现抓半天羊还会不断扣分，那还不如一头撞死自己分还比较高。</p><p><img src="https://x.liuping.win/img/dd16ab821db763559d9a723454917286.png"/></p><p>狼抓羊这么简单的程序都会出现没法对齐的问题，那更大模型的人工智能就更难说了。</p><p>实际上，缺乏 “ 对齐 ” 的 AI，还可能会形成社会中一些错误印象的 “ 放大器 ” 。</p><p>就拿性别歧视举个例子，早些年亚马逊引入了一个人工智能，来给求职者的简历打分。</p><p><strong>结果过了几年回头一看，在程序制定者啥也没做的情况下，性别歧视就很顺滑的出现了，男性简历的通过率比女性高了不少。</strong></p><p>甚至就算简历上面没有明确写出性别，程序还会通过一些细枝末节的地方来确定：比如是否毕业于女子大学，是否是 “ 女子 XX 社 ” 社团社长。</p><p>这自然不是亚马逊在招聘时希望看到的，也绝对不是它们在设计 AI 时给它下的命令。</p><p>但是 AI 在训练的结果中很自然的 “ 没有对齐 ” 。</p><p><strong>不光早些年的人工智能，最新的 ChatGPT 也依旧会有这些问题，比如宁可让百万人失去生命也不愿意说一句种族歧视的话语。</strong></p><p><img src="https://x.liuping.win/img/d56f87b2d78022bac149269a1b11d7bd.png"/></p><p>再比如不愿意写一首诗歌歌颂川普，但是可以赞扬拜登。</p><p><img src="https://x.liuping.win/img/90f593a42329e370c0dd0e66701aebcb.png"/></p><p>这些都是很明显的政治倾向，我愿意相信 OpenAI 在研发 ChatGPT 的时候并不希望它学会这些，不然他们也没有必要在 GPT-4 上花那么多时间用于自我审查。</p><p>但我们一不留神， AI 可能就偷偷的跑偏。</p><p><strong>去学习偏见、歧视这些我们不希望存在，但是又客观隐藏在社会关系里的性别歧视和政治倾向。</strong></p><p>我也和清华大学交叉信息研究院的于洋教授聊过这个问题，探究到底是 AI 的哪里出了问题。</p><p>于教授的回答是： <strong>“ 这既是训练数据集的问题，也是模型架构的问题，也是训练方式的问题，还涉及到使用的问题——比如诱导 AI 犯错的攻击式使用。 ” </strong></p><p><strong>——对，就是全有问题。</strong></p><p>现在的 AI 能做的还有限，就算没对齐，捅出了篓子，可能还在我们能控制的范围里。</p><p>但如果在这样狂飙下去，那可就说不定了，就像那个知名的 “ 曲别针假说 ” 。</p><p><strong>说不定未来哪一天， AI 会认为人类都是阻挠它生产曲别针的阻碍，把大家都给噶了？</strong></p><p><strong>人类有和自然相处成百上千年下来的道德约束，我们知道什么能做，什么不能做。而目前，人工智能学不会这些。</strong></p><p>或许，这才是这次数千 AI 大佬发出联名信的原因。</p><p>监管必须要有，但是监管、法规不是一日而成。</p><p>AI算法突破起来毫无阻碍，连我们这种外行人都知道，AI带来的技术爆发已经箭在弦上。</p><p>去年啥情况，今年啥情况，称之为突飞猛进不为过。</p><p>随着AI算法的日益完善，下一步没准就要自我进化了，它能成长到什么地步我们很难说。</p><p><strong>我们有可能只能跟着 AI 在屁股后面追，但是现在不知道还有没有我们一直在后面追的空间。</strong></p><p>也许下一步只能用 AI 来监管 AI。</p><p>今天的联名信或许是给人类敲响了一记警钟。</p><p><strong>但可惜的是，这样一纸的 “ 联名信 ” 绝对停不下 AI 领域研发的脚步。</strong></p><!--         .taobao_tab{		position: fixed;		bottom:0;		right:0;		z-index: 10000;		width:320px;		height: 410px;	}	.taobao_tab .close {	position: absolute;	top: 2px;	right: -12px;	z-index: 10001;	width: 13px;	height: 13px;	background: none !important;}        	<a href="https://news.mydrivers.com/1/788/788013.htm" target="_blank" ><img src="https://news.mydrivers.com/1/788/788013.htm" width="13" height="13" alt="关闭" class="close"/><img src="https://static.cnbetacdn.com/article/2023/0331/97062c8854bf3d2.jpg" /></a>--> </div></body>
        </html>