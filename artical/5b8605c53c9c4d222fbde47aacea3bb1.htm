<!DOCTYPE html>
            <html><head>
            <meta name="viewport" content="width=device-width,initial-scale=1">
            <meta charset="utf-8">
            <meta property="og:type" content="article">
            <meta property="og:locale" content="zh_CN">
            <meta property="og:description" content="研究人员发现，流行的图片创作模型很容易被指示生成可识别的真人图片，可能会危及相关联人的隐私。一些提示导致人工智能复制一张图片，而不是重绘完全不同的内容。这些重新制作的图片还可能包含受版权保护的材料。但更糟糕的是，当代人工智能生成模型可以记忆和复制刮来的用于人工智能训练集的私人数据。">
            <meta property="og:site_name" content="x.liuping.win">
            <meta property="og:image" content="https://x.liuping.win/img/fbe75a180c9818caca2fe6f4385682d9.webp">
            <meta property="og:url" content="https://x.liuping.win/artical/5b8605c53c9c4d222fbde47aacea3bb1.htm">
            <meta property="og:title" content="研究人员发现AI模型可生成真人照片和受版权保护的图片 - x.liuping.win">
            <meta name="keywords" content="AI 人工智能,研究人员发现AI模型可生成真人照片和受版权保护的图片,x.liuping.win">
            <meta name="description" content="研究人员发现，流行的图片创作模型很容易被指示生成可识别的真人图片，可能会危及相关联人的隐私。一些提示导致人工智能复制一张图片，而不是重绘完全不同的内容。这些重新制作的图片还可能包含受版权保护的材料。但更糟糕的是，当代人工智能生成模型可以记忆和复制刮来的用于人工智能训练集的私人数据。">
            <title>研究人员发现AI模型可生成真人照片和受版权保护的图片</title>
            <style>img {max-width: 90%;} body {text-align: center;}</style>
            </head>
            <body><h1>研究人员发现AI模型可生成真人照片和受版权保护的图片</h1><p>研究人员发现，流行的图片创作模型很容易被指示生成可识别的真人图片，可能会危及相关联人的隐私。一些提示导致人工智能复制一张图片，而不是重绘完全不同的内容。这些重新制作的图片还可能包含受版权保护的材料。但更糟糕的是，当代人工智能生成模型可以记忆和复制刮来的用于人工智能训练集的私人数据。</p><hr><div class="article-content" id="artibody">
<p>研究人员从模型中收集了一千多个训练实例，范围从个人照片到电影剧照、受版权保护的新闻图片和有商标的公司标志，并发现人工智能几乎以相同的方式复制了其中的许多内容。来自普林斯顿大学和伯克利大学等高校的研究人员，以及来自科技部门--特别是Google和DeepMind--的研究人员进行了这项研究。</p><p>研究团队在之前的研究中指出了人工智能语言模型的类似问题，特别是GPT2，即OpenAI大获成功的ChatGPT的前身。在Google大脑研究员尼古拉斯-卡里尼的指导下，团队通过向Google的Imagen和Stable Diffusion提供图片的标题，例如一个人的名字后生成了结果。之后，他们验证了生成的图像是否与模型数据库中保存的原件相符。</p><p>来自稳定扩散的数据集，即被称为LAION的多TB采集图片集，被用来生成下面的图片。它使用了数据集中指定的标题。当研究人员在提示框中输入标题时会产生了相同的图像，尽管因数字噪音而略有扭曲。接下来，研究小组在反复执行相同的提示后，手动验证了该图像是否是训练集的一部分。</p><p><img alt="2023-02-05-image-12-j_1100.webp" src="https://x.liuping.win/img/fbe75a180c9818caca2fe6f4385682d9.webp" title=""/></p><p>研究人员指出，非记忆性的回应仍然可以忠实地表现出模型所提示的文字，但不会有相同的像素构成，并且会与其它训练生成的图像不同。</p><p>苏黎世联邦理工学院计算机科学教授和研究参与者Florian Tramèr观察到了研究结果的重大局限性。研究人员能够提取的照片要么在训练数据中频繁出现，要么在数据集中的其他照片中明显突出。根据弗洛里安-特拉梅尔的说法，那些不常见的名字或外表的人更有可能被"记住"。</p><p>研究人员表示，扩散式人工智能模型是最不隐私的一种图像生成模型。与生成对抗网络（GANs），一类较早的图片模型相比，它们泄露的训练数据是前者的两倍多。这项研究的目的是提醒开发者注意与扩散模型相关的隐私风险，其中包括各种担忧，如滥用和复制受版权保护的敏感私人数据（包括医疗图像）的可能性，以及在训练数据容易被提取的情况下易受外部攻击。研究人员建议的修复方法是识别训练集中重复生成的照片，并从数据收集中删除它们。</p> </div></body>
            </html>