<!DOCTYPE html>
            <html><head>
            <meta name="viewport" content="width=device-width,initial-scale=1">
            <meta charset="utf-8">
            <meta property="og:type" content="article">
            <meta property="og:locale" content="zh_CN">
            <meta property="og:description" content="人类通过不同模式的组合来观察世界，如视觉、听觉和我们对语言的理解。另一方面，机器则通过算法可以处理的数据来解释世界。因此，当机器&quot;看到&quot;一张照片时，它必须将照片编码为它可以用来执行图像分类等任务的数据。当输入有多种格式，如视频、音频片段和图像时，这个过程变得更加复杂。">
            <meta property="og:site_name" content="x.liuping.win">
            <meta property="og:image" content="https://x.liuping.win/img/6e07b9804651add242e850ca1372d5d2.webp">
            <meta property="og:url" content="https://x.liuping.win/artical/7044a95680c4436fe8e0ba33871c3aae.htm">
            <meta property="og:title" content="革命性的人工智能系统会学习视频、音频和文本中的共享概念 - x.liuping.win">
            <meta name="keywords" content="AI 人工智能,革命性的人工智能系统会学习视频、音频和文本中的共享概念,x.liuping.win">
            <meta name="description" content="人类通过不同模式的组合来观察世界，如视觉、听觉和我们对语言的理解。另一方面，机器则通过算法可以处理的数据来解释世界。因此，当机器&amp;quot;看到&amp;quot;一张照片时，它必须将照片编码为它可以用来执行图像分类等任务的数据。当输入有多种格式，如视频、音频片段和图像时，这个过程变得更加复杂。">
            <title>革命性的人工智能系统会学习视频、音频和文本中的共享概念</title>
            <style>img {max-width: 90%;} body {text-align: center;}</style>
            </head>
            <body><h1>革命性的人工智能系统会学习视频、音频和文本中的共享概念</h1><p>人类通过不同模式的组合来观察世界，如视觉、听觉和我们对语言的理解。另一方面，机器则通过算法可以处理的数据来解释世界。因此，当机器"看到"一张照片时，它必须将照片编码为它可以用来执行图像分类等任务的数据。当输入有多种格式，如视频、音频片段和图像时，这个过程变得更加复杂。</p><hr><div class="article-content" id="artibody">
<p>"这里的主要挑战是，机器如何能将这些不同的模式对齐？作为人类，这对我们来说很容易。我们看到一辆汽车，然后听到汽车驶过的声音，我们知道这些是同一件事。但是对于机器学习来说，这就不是那么简单了。"计算机科学与人工智能实验室（CSAIL）的研究生、解决这一问题的论文的第一作者Alexander Liu说。</p><p><img alt="Artificial-Intelligence-System-Video-Audio-Text-1536x1023.webp" src="https://x.liuping.win/img/6e07b9804651add242e850ca1372d5d2.webp" title=""/></p><p>Liu和他的合作者开发了一种人工智能技术，可以学习以一种捕捉视觉和听觉模式之间共享的概念的方式来表示数据。例如，他们的方法可以学习到视频中婴儿哭泣的动作与音频片段中的口语单词"哭泣"有关。</p><p>利用这些知识，他们的机器学习模型可以识别视频中某个动作发生的位置，并对其进行标注。</p><p>它在跨模式检索任务方面比其他机器学习方法表现得更好，跨模式检索任务涉及寻找一段数据，如视频，与用户以另一种形式（如口头语言）给出的查询相匹配。他们的模型也使用户更容易看到为什么机器认为它检索的视频与他们的查询相匹配。</p><p>这项技术有朝一日可以被用来帮助机器人通过感知来学习世界上的概念，更像人类的方式。</p><p>与Liu一起撰写论文的还有CSAIL的博士后SouYoung Jin；研究生Cheng-I Jeff Lai和Andrew Rouditchenko；CSAIL的高级研究科学家和麻省理工学院-IBM Watson人工智能实验室主任Aude Oliva；以及高级作者James Glass，CSAIL的高级研究科学家和口语系统组负责人。这项研究将在计算语言学协会的年会上发表。</p><p><strong>学习表征</strong></p><p>研究人员将工作重点放在表征学习上，这是机器学习的一种形式，旨在转换输入数据，使其更容易执行分类或预测等任务。</p><p>表征学习模型采用原始数据，如视频及其相应的文字说明，并通过提取特征，或对视频中的物体和行动的观察对它们进行编码。然后，它将这些数据点映射到一个网格中，即所谓的嵌入空间。该模型将类似的数据作为网格中的单个点聚在一起。这些数据点中的每一个，或称向量，都由一个单独的词来表示。</p><p>例如，一个人玩杂耍的视频片段可能被映射到一个标有"杂耍"的向量。</p><p>研究人员对该模型进行了限制，使其只能使用1000个词来标记向量。该模型可以决定它想把哪些动作或概念编码到一个矢量中，但它只能使用1000个矢量。模型会选择它认为最能代表数据的词。</p><p>他们的方法不是将来自不同模式的数据编码到不同的网格上，而是采用了一个共享的嵌入空间，在这个空间里，两种模式可以一起编码。这使该模型能够学习两种模式的表征之间的关系，比如显示一个人在玩杂耍的视频和一个人说"杂耍"的音频记录。</p><p>为了帮助系统处理来自多种模式的数据，他们设计了一种算法，引导机器将类似的概念编码到同一个向量中。"如果有一个关于猪的视频，模型可能会把'猪'这个词分配到1000个向量中的一个。然后，如果模型听到有人在音频片段中说'猪'这个词，它应该仍然使用同一个向量来编码，"Liu解释说。</p><p><strong>更好的检索器</strong></p><p>他们使用三个数据集对该模型进行了跨模式检索任务的测试：一个包含视频片段和文字说明的视频-文本数据集，一个包含视频片段和语音说明的视频-音频数据集，以及一个包含图像和语音说明的图像-音频数据集。</p><p>例如，在视频-音频数据集中，该模型选择了1000个词来代表视频中的动作。然后，当研究人员向其提供音频查询时，该模型试图找到与这些口语最匹配的片段。</p><p>"就像Google搜索一样，你输入一些文字，机器就会试图告诉你你正在搜索的最相关的东西。只不过我们是在矢量空间里做这个工作，"Liu说。</p><p>他们的技术不仅比他们所比较的模型更有可能找到更好的匹配，而且也更容易理解。</p><p>因为该模型只能使用总共1000个词来标记向量，用户可以更容易地看到机器用哪些词来得出视频和口语相似的结论。Liu说，这可能使该模型更容易应用于现实世界的情况，在这种情况下，用户理解它是如何做出决定的，这一点至关重要。</p><p>该模型仍有一些局限性，他们希望在未来的工作中加以解决。Liu说，首先，他们的研究集中在一次来自两种模式的数据，但在现实世界中，人类同时遇到许多数据模式。</p><p>"而且我们知道1000个字在这种数据集上是有效的，但我们不知道它是否可以被推广到现实世界的问题上，"他补充说。</p><p>此外，他们的数据集中的图像和视频包含简单的物体或直接的动作；现实世界的数据要混乱得多。他们还想确定他们的方法在有更广泛的输入多样性的情况下的扩展性如何。</p> </div></body>
            </html>