<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="3月28日，GPT-4发布两周后，一封埃隆·马斯克（ElonMusk）以及上千名科技界人士签署的公开信在网上发布。这封公开信呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练至少6个月。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/57c7a33d00469edbf1d4a66c8272c254.png">
        <meta property="og:url" content="https://x.liuping.win/artical/50d9b045487198be0420e4d5b07316ed.htm">
        <meta property="og:title" content="为什么马斯克、图灵奖得主等千名从业者联名叫停大型AI研究？ - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,为什么马斯克、图灵奖得主等千名从业者联名叫停大型AI研究？,x.liuping.win">
        <meta name="description" content="3月28日，GPT-4发布两周后，一封埃隆·马斯克（ElonMusk）以及上千名科技界人士签署的公开信在网上发布。这封公开信呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练至少6个月。">
        <title>为什么马斯克、图灵奖得主等千名从业者联名叫停大型AI研究？</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>为什么马斯克、图灵奖得主等千名从业者联名叫停大型AI研究？</h1><p>3月28日，GPT-4发布两周后，一封埃隆·马斯克（Elon Musk）以及上千名科技界人士签署的公开信在网上发布。这封公开信呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练至少6个月。</p><hr><div class="article-content" id="artibody">
<p><img src="https://x.liuping.win/img/57c7a33d00469edbf1d4a66c8272c254.png"/></p><p>苹果联合创始人史蒂夫·沃兹尼亚克（Steve Wozniak）、图灵奖得主约书亚·本吉奥（Yoshua Bengio）、AI领域教科书《人工智能：现代方法》的合著者斯图尔特·罗素（Stuart Russell）等著名科技人士均在这封公开信上署名。</p><p>这封公开信声称，“高级AI可能意味着地球生命史上的深刻变革，我们应当投入相称的关注和资源对其进行规划和管理”，但现在人工智能已经陷入失控的竞赛，开发者也无法理解、预测及可靠地控制。</p><p>这封公开信还提到，当代人工智能系统现在在一般任务上变得与人类具有竞争力，“只有当我们确信强大的人工智能系统的效果是积极的，其风险是可控的，才应该开发。”</p><p>按照公开信的设想，应该暂停比 GPT-4 更强大的 AI 系统6个月训练。在这段暂停期间，人工智能实验室和独立专家应该共同开发和实施一套用于高级人工智能设计和开发的共享安全协议，并由独立的外部专家进行严格审计和监督。</p><p>这些协议应确保它们的系统是安全的，并且无可置疑。这并不意味着总体上暂停AI开发，只是从向更大的、不可预测的黑盒模型的危险竞赛中退后一步。</p><p>这封信发布在未来生命研究所 (Future of Life Institute) 网站上，那家研究所主要由马斯克基金会、有效利他主义组织 Founders Pledge和硅谷社区基金会资助，机构的使命是引导变革性技术远离极端、大规模的风险，转向造福生活。</p><p>早在2017年，未来生命研究所便曾在加州召集了100 多位经济学、法律、伦理学和哲学领域的思想领袖和研究人员齐聚一堂，讨论和制定有益人工智能的原则。最终他们制定了23条阿西洛马人工智能原则，被视为人工智能治理的重要原则。其中第一条是“ 人工智能研究的目标应该建立有益的智能，而不是无秩序的的智能。”</p><p>对本次在公开信上署名的那些科技工作者来说，大部分人的担忧在于：AI发展的太快了，相关的监管和法律没有跟上，甚至连发明者也缺乏有效的控制手段。这种条件下，无限制地使用AI，如果出现问题，很可能就会带来系统性的风险。</p><p>根据《时代周刊》报道，公开信的签署人、人工智能安全初创公司 SaferAI 的首席执行官西蒙·坎波斯表示，AI系统的发明者不确切知道它们是如何工作的，也不知道它们是什么，因此无法管理系统的风险。发明者有能力做到，但不知道如何限制AI的行为。</p><p>“我们现在在做什么？” 坎波斯说。“我们正在全速将这些系统扩展到前所未有的能力水平，并对社会产生变革性影响。我们必须放慢这些系统的开发速度，让社会适应。”</p><p><strong>马斯克的担心：关不掉的AI</strong></p><p>马斯克对AI的警惕由来已久，早在2014年，马斯克曾在一次采访中公开表示，我们应该对人工智能非常小心，如果一定要猜人类生存的最大威胁是什么，那可能就是人工智能。“通过人工智能，人类正在召唤恶魔”，马斯克说。</p><p>他还表示“我越来越倾向于认为应该进行一些监管监督，也许是在国家和国际层面，只是为了确保我们不会做一些非常愚蠢的事情。”</p><p>马斯克对AI安全问题不只是说说而已。2015年初，马斯克向未来生命研究所捐助700万美元的资助，旨在让AI造福于人类的研究。被资助者大多从事AI伦理、治理、安全之类的研究，很多受资助者，比如斯图尔特·罗素，参与了这次公开信署名。</p><p>作为DeepMind的早期投资者，马斯克对AI的态度似乎有些矛盾。但他声称自己这项投资并非从投资回报率角度出发，只是想关注人工智能的发展，因为这可能导向危险的结果。他说，有一些可怕的后果，我们应该努力确保结果是好的，而不是坏的。”</p><p>坏的结果并不是像电影里描绘的那样，机器人造反消灭人类。相反，专家们不担心机器人的意识，而担心它们的能力。AI实现目的的能力强大，但如果失控或者脱离计划中的使用场景，就可能造成不可逆转的破坏。</p><p>在公开信上签名的重要人物，图灵奖获得者，被誉为“AI教父”的约书亚·本吉奥一直关注人工智能的滥用问题。2018 年，本吉奥是签署反对AI武器发展承诺书的数千名人工智能研究人员之一。他呼吁各国政府规范人工智能研究，停止致命机器人的发展。</p><p>AI武器便是个机器人被滥用的极端例子，AI武器用于战场，会比传统武器更有效率。不过一旦失控，人工智能武器造成的问题也会更严重。在战场上，为了不在简单干扰下失灵，现在的AI武器往往会设计得很难被直接关闭。设想一下，如果被编程为攻击人类的高效AI武器意外落在平民居住区，会产生什么样的惨痛后果。</p><p>事实上，如何关掉AI，是很多签名大佬担心的问题。</p><p>但如果AI的应用走得太快，关掉它也许便可能需要付出巨大代价，成了一件难事。马斯克便曾设想过，如果一个系统里算法失控了，管理人员可以找到它。但如果这个系统是大型AI管理的，我们可能根本找不到失控的位置，或者没有权限停止整个大型的AI的运作。简单的检修会成为难题。</p><p>这也就是为什么AI需要更多监管，需要在技术上更民主，而不是集中在大公司的手中。</p><p>为了让AI的力量不集中于大公司手中，尤其是Google DeepMind手中，马斯克和山姆・阿尔特曼(Sam Altman) 合作创立了OpenAI ，目的是实现是关于人工智能权力的民主化，降低 AI 力量被垄断的可能性。</p><p>这多少有些讽刺，由OpenAI所创造的GPT-4，正是马斯克现在着力反对的。就在几天前，马斯克还在和OpenAI的CEO阿特尔曼（Sam Altman）打隔空嘴仗。</p><p><strong>停下来，“我们对齐一下”</strong></p><p>这些大佬们的担忧之外，在更广泛的民众中，对于AI的恐惧则由来已久。</p><p>这其中，有对科幻电影《太空漫游2001》中那种获得了意识的超智慧电脑HAL 9000的恐惧，也有更实际的，对于AI控制世界可能会造成的混乱和糟糕局面的恐惧。</p><p>在那封公开信中，多次被引用的一份资料是被包括微软高管在内的多位人工智能领域大佬盛赞的畅销书《对齐问题》（Alignment Problem）。发布那封公开信的未来生命研究所（FLI）一位联合联合创始人便曾公开称赞那本书：“充满了令人惊艳的发现，意想不到的障碍、巧妙的解决方案，以及越来越多关于我们这个物种本质的难题”。而那本书中所讨论的最大的难题便是：人工智能伦理。</p><p>一个最简单的人工智能伦理问题是曲别针问题，如果命令一个机器人去造尽可能多的曲别针，只要可能，它就会穷尽地球上所有的资源，不知疲倦地去制造曲别针，因为完成这个任务与它的道德规范没有什么冲突，那么它能做的就只能是执行任务。人类在漫长的历史中，大量固化在我们文化中，深植于每一个人的头脑中的道德规范，我们的敬与畏，有时我们自己都尚未察觉，而一个机器，并没有经历过这样漫长的进化，如同一个生下来就有超能力的孩童，它们自己就变成了风险。</p><p>因而，《对齐问题》一书的作者，现在正在加州大学伯克利分校做访问学者的畅销书作家布莱恩（Brian Christian）认为，需要细致、全面以及实时地让AI的道德规范与人类的道德规范进行对齐。从这个意义上，这封公开信，便如同大厂的某个部门，要求员工停止手中的工作，都到会议室去，“我们对齐一下”。</p><p>当然，《对齐问题》除了讨论这些看起来比较遥远的问题，也有正在发生的。</p><p>美国某州利用计算机程序为罪犯的再次犯罪概率打分，进而决定保释与假释，分配保证金数额。然而，这个系统发生了些奇怪的事情，黑人博登被评为高风险，白人普拉特则被评为低风险，对两人的两年跟踪调查发现，两年间，博登再也没有受到任何犯罪指控，而普拉特则因抢劫盗窃被判入狱8年。很显然，大数据之下，人类中的种族歧视也被移植到了AI中——但关键问题在于，所有的人都认为，他们是初生的婴儿，是公平。在《对齐问题》中，作者还提到了招聘中的性别歧视，没有人对AI去做什么设定，然而大数据之下，歧视自然的就产生了。</p><p>基于大数据建立的人工智能，也许会强化我们族群中被广泛忽略的歧视问题、不平等问题，而一旦这些问题在未被察觉，或者被认为微不足道的情况下进入系统，它们就被固化了，这会是一个很不美好的结局。</p><p>于是，读者们对《对齐问题》一书的评论中，很多人提到了人——AI在放大人类现存的问题，以及，最为重要的，AI将交到什么样的人手里？</p><p>几乎毫无疑问的，AI会带来生产力的大幅增长，但与上一场信息革命一样，这种强大的劳动力工具会进一步导致财富的集中——从1980年到今天，收入最高的1%的人口所占的收入份额已经从10%上升到了近20%，而最底层的50%的人口所占的收入份额已经从1980年的20%下降到了12%。没人想象的出，GPT面世40年之后，这个数据会变成怎样？更何况这是一个更亲资本的生产力工具。</p><p>早在今年2月份，长期关注人工智能问题的MIT教授达龙（Daron Acemoglu）就曾发文提到下列场景：企业辞退了人工客服，大量的人失业，而消费者只能接受一个能言善辩却不能解决任何问题的机器人客服的服务。这样的方式，“会剥夺和取代员工的权力并降低消费者体验，最终让大多数投资者失望”，我们不该如此。</p><p>这次公开信并不是人们第一次对人工智的未来表示担忧，盖茨在他的公开信中同样曾提到过要关注人工智能时代的平等问题：市场力量不会自然而然地生产出帮助最贫困人群的人工智能产品和服务，相反的可能性反倒会更大。</p><p>早在3月16日，阿特尔曼同样曾担忧地提到“能够思考和学习的软件将完成越来越多的人们现在所做的工作。更多的权力将从劳动力转移到资本。如果公共政策不做出相应调整，大多数人最终会过得比现在更糟”。</p><p>因此，阿特尔曼提到，需要引入新的体系，向资本而非劳动者征税从而让更多的人能够共享这次人工智能革命的成果。</p><p>目前，AI要成为人类生活的基础设施还需要一段时间，但这个时间可能已经不多了。GPT-4出现后，大型AI的能力在”军备竞赛“中飞涨，AI应用可能会快就会普及到生活的各个方面，却没有时间帮AI测试安全程度、理解人类的需求、设定监管的方案。这也是这次公开信背后的专家们，要为大型AI研究叫暂停的原因。</p><p>也许，如同微软首席科学家Eric Horvitz所言：今天，在地平线上的人工智能，有些事情是已知的，有些事情是未知的，中间则是为我们观察这个世界留下的大门。</p> </div></body>
        </html>