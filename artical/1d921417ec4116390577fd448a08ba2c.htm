<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="当全社会都在为AI的进化而欢呼雀跃，全新的、意想不到的风险可能也正在悄悄酝酿。3月22日，生命未来研究所（FutureofLife）向全社会发布了一封《暂停大型人工智能研究》的公开信，呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练，暂停时间至少为6个月。该机构以“引导变革性技术造福生活，远离极端的大规模风险”为使命。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/749218ddf5cb9d6b9c9fcb7d868f1486.webp">
        <meta property="og:url" content="https://x.liuping.win/artical/1d921417ec4116390577fd448a08ba2c.htm">
        <meta property="og:title" content="1000多名硅谷企业家科学家联名呼吁立刻停下所有大型AI研究 - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,1000多名硅谷企业家科学家联名呼吁立刻停下所有大型AI研究,x.liuping.win">
        <meta name="description" content="当全社会都在为AI的进化而欢呼雀跃，全新的、意想不到的风险可能也正在悄悄酝酿。3月22日，生命未来研究所（FutureofLife）向全社会发布了一封《暂停大型人工智能研究》的公开信，呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练，暂停时间至少为6个月。该机构以“引导变革性技术造福生活，远离极端的大规模风险”为使命。">
        <title>1000多名硅谷企业家科学家联名呼吁立刻停下所有大型AI研究</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>1000多名硅谷企业家科学家联名呼吁立刻停下所有大型AI研究</h1><p>当全社会都在为AI的进化而欢呼雀跃，全新的、意想不到的风险可能也正在悄悄酝酿。3月22日，生命未来研究所（Future of 
Life）向全社会发布了一封《暂停大型人工智能研究》的公开信，呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练，暂停时间至少为6个月。该机构以“引导变革性技术造福生活，远离极端的大规模风险”为使命。</p><hr><div class="article-content" id="artibody">
<p><img src="https://x.liuping.win/img/749218ddf5cb9d6b9c9fcb7d868f1486.webp"/><br/></p><p>该机构在信中提到：</p><p>我们不应该冒着失去对文明控制的风险，将决定委托给未经选举的技术领袖。只有当确保强大的人工智能系统的效果是积极的，其风险是可控的才能继续开发。</p><p>人工智能实验室和独立专家应在暂停期间，共同制定和实施一套先进的人工智能设计和开发的共享安全协议，由独立的外部专家进行严格的审查和监督。</p><p>截止目前，马斯克、苹果联合创始人Steve Wozniak、Stability AI 创始人Emad Mostaque等上千名科技大佬和AI专家已经签署公开信。</p><p><img src="https://x.liuping.win/img/53ff1599fec5c0c82db46366bc5b2bfa.webp"/><br/></p><p>值得一提的是，OpenAI CEO奥特曼在与MIT研究科学家Lex Fridman的最新对话中指出，AI已经出现其无法解释的推理能力，同时承认“AI杀死人类”有一定可能性。</p><p>那么ChatGPT是如何看待公开信中AI对人类社会的影响？华尔街见闻专门询问了ChatGPT 3.5，其表示随着人工智能技术的不断发展和普及，人工智能可能对社会和人类造成的影响将越来越大。这封公开信提出的呼吁和建议，应该被视为引起更广泛和深入讨论的起点和参考，而不是一个终点或解决方案。</p><p><img src="https://x.liuping.win/img/2fe70d6903c02aa96918fb3b0c6caea9.webp"/><br/></p><p>以下是公开信原文：</p><p>正如广泛的研究和顶级人工智能实验室所承认的，人工智能系统对社会和人类构成较大的风险。《阿西洛马人工智能原则》中指出，高级人工智能可能代表着地球上生命历史的深刻变革，应该以相应的谨慎和资源进行规划和管理。然而，尽管最近几个月人工智能实验室掀起AI狂潮，开发和部署越来越强大的数字大脑，但目前没有人能理解、预测或可靠地控制AI系统，也没有相应水平的规划和管理。</p><p>现在，人工智能在一般任务上变得与人类有竞争力，我们必须问自己：我们应该让机器在信息渠道中宣传不真实的信息吗？我们是否应该把所有的工作都自动化，包括那些有成就感的工作？我们是否应该开发非人类的大脑，使其最终超过人类数量，胜过人类的智慧，淘汰并取代人类？我们是否应该冒着失去对我们文明控制的风险？这样的决定绝不能委托给未经选举的技术领袖。只有当我们确信强大的人工智能系统的效果是积极的，其风险是可控的，才应该开发。同时，这种信心必须得到验证，并随着系统的潜在影响的大小而加强。OpenAI最近关于人工智能的声明中指出，在开始训练未来的系统之前，可能必须得到独立的审查，对于最先进的努力，同意限制用于创建新模型的计算增长速度。我们同意，现在就该采取行动。</p><p>因此，我们呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练，时间至少持续6个月。这种暂停应该是公开的、可核查的，并包括所有关键参与者者。如果这种禁令不能迅速实施，政府应该介入并制定暂停令。</p><p>人工智能实验室和独立专家应在暂停期间，共同制定和实施一套先进的人工智能设计和开发的共享安全协议，由独立的外部专家进行严格的审查和监督。这些协议应该确保遵守协议的系统是安全的。值得一提的是，这并不意味着暂停一般的人工智能开发，只是从危险的竞赛中退后一步，限制不可预测的研究和开发。</p><p>人工智能的研究和开发应该重新聚焦于，使目前最先进和强大的系统更加准确、安全、可解释、透明、稳健、一致、值得信赖和忠诚。</p><p>同时，人工智能开发者必须与政策制定者合作，大幅加快开发强大的人工智能治理系统。这些至少应该包括：专门针对人工智能的监管机构；监督和跟踪高能力的人工智能系统和大型计算能力的硬件；出处和水印系统帮助区分真实和合成，并跟踪模型泄漏；强大的审查和认证生态系统；对人工智能造成的伤害承担责任；为人工智能安全技术研究提供强大的公共资金以及资源充足的机构，以应对人工智能可能导致的巨大的经济和政治破坏。</p><p>人类可以通过人工智能享受繁荣的未来。现在，我们成功地创建了强大的人工智能系统，可以在这个“人工智能之夏”中获得回报，为所有人的明确利益设计这些系统，并给社会一个适应的机会。停止使用其他技术可能会对社会造成灾难性影响，因此我们必须保持准备状态。让我们享受一个漫长的人工智能夏天，而不是匆忙进入秋天。</p><p><img src="https://x.liuping.win/img/4613d7095bfe16b861d2345c2dd054fc.webp"/><br/></p> </div></body>
        </html>