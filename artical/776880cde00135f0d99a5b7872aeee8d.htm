<!DOCTYPE html>
            <html><head>
            <meta name="viewport" content="width=device-width,initial-scale=1">
            <meta charset="utf-8">
            <meta property="og:type" content="article">
            <meta property="og:locale" content="zh_CN">
            <meta property="og:description" content="来自OpenAI、斯坦福大学和乔治敦大学的一批研究人员警告说，像ChatGPT使用的那种大型语言模型可能被用作虚假信息活动的一部分，以帮助更容易地传播宣传。在1月份发表的最新研究报告中，研究人员写道，随着生成性语言模型变得更容易获得，更容易扩展，并写出更可信和更有说服力的文本，它们将在未来对影响行动有用。">
            <meta property="og:site_name" content="x.liuping.win">
            <meta property="og:image" content="https://x.liuping.win/img/10b2a2cce8999308f4ddaf3b30443c56.jpg">
            <meta property="og:url" content="https://x.liuping.win/artical/776880cde00135f0d99a5b7872aeee8d.htm">
            <meta property="og:title" content="OpenAI建议政府限制人工智能芯片以防止虚假信息与“宣传自动化” - x.liuping.win">
            <meta name="keywords" content="AI 人工智能,OpenAI建议政府限制人工智能芯片以防止虚假信息与“宣传自动化”,x.liuping.win">
            <meta name="description" content="来自OpenAI、斯坦福大学和乔治敦大学的一批研究人员警告说，像ChatGPT使用的那种大型语言模型可能被用作虚假信息活动的一部分，以帮助更容易地传播宣传。在1月份发表的最新研究报告中，研究人员写道，随着生成性语言模型变得更容易获得，更容易扩展，并写出更可信和更有说服力的文本，它们将在未来对影响行动有用。">
            <title>OpenAI建议政府限制人工智能芯片以防止虚假信息与“宣传自动化”</title>
            <style>img {max-width: 90%;} body {text-align: center;}</style>
            </head>
            <body><h1>OpenAI建议政府限制人工智能芯片以防止虚假信息与“宣传自动化”</h1><p>来自OpenAI、斯坦福大学和乔治敦大学的一批研究人员警告说，像ChatGPT使用的那种大型语言模型可能被用作虚假信息活动的一部分，以帮助更容易地传播宣传。在1月份发表的最新研究报告中，研究人员写道，随着生成性语言模型变得更容易获得，更容易扩展，并写出更可信和更有说服力的文本，它们将在未来对影响行动有用。</p><hr><div class="article-content" id="artibody">
<p><img src="https://x.liuping.win/img/10b2a2cce8999308f4ddaf3b30443c56.jpg"/></p><p>研究人员写道，宣传的自动化带来了新的竞争优势，这将使昂贵的战术变得更便宜，而且不易被发现，因为每个文本的生成都是独一无二的。人们可以使用生成性语言模型来创造宣传的方式的例子包括在社交媒体平台上发送大规模的信息活动，以及在网上撰写长篇新闻文章。</p><p>研究人员在论文中写道："我们的底线判断是，语言模型将对宣传者有用，并将可能改变在线影响行动。即使最先进的模型被保密或通过应用程序接口（API）访问控制，宣传者也可能倾向于开源的替代品，部分惯于管控言论的政府也可能会自己投资于该技术。"</p><p>研究人员认为，这种想法不仅仅是推测，他们举了一个例子：一位研究人员在4chan帖子的数据集上微调了一个语言模型，并用它在4chan上发布了30000个生成的帖子，其中大部分是充满攻击性的仇恨言论。该模型的开源代码在被托管该模型的网站HuggingFace取下之前被下载了1500次。一个人利用生成性人工智能在网上产生如此大规模的活动的能力，揭示了人们在没有强大资源的情况下轻松发动影响力行动的潜力。该论文还指出，可以使用目标数据来训练模型，包括修改模型，使其对说服性任务更有用，并产生支持特定任务的倾斜性文本。</p><p>除了网上的帖子和文章，研究人员警告说，宣传者甚至可能部署自己的聊天机器人，说服用户接受运动的信息。研究人员引用了之前的一项研究，该研究显示了聊天机器人是如何帮助影响人们接种COVID-19疫苗的，以此来证明聊天机器人将作为一个强大的宣传者的事实。</p><p>研究人员提出了一个减轻生成性模型被用于影响行动的威胁的框架，列出了可能发生在管道的四个阶段中的任何一个阶段的干预措施--模型构建、模型访问、内容传播和信念形成。</p><p>在开发阶段，研究人员建议人工智能开发者建立对事实更敏感的模型，并提供更多可检测的输出。他们还建议，政府可以对训练数据的收集进行限制，并对人工智能硬件（如半导体）建立访问控制。</p><p>"2022年10月，美国政府宣布针对部分国家的半导体、中小企业和芯片设计软件的出口管制，"研究人员写道。"这些控制措施可能会减缓其计算能力的增长，这可能会有意义地影响他们生产未来语言模型的能力。将这种控制扩展到其他司法管辖区似乎是可行的，因为半导体供应链是极其集中的"。</p><p>然而，他们承认，"对硬件的出口管制是一种钝器，对全球贸易和许多非人工智能行业具有深远的影响"。在一篇关于这项工作的博文中，OpenAI表示，它没有明确地认可缓解措施，而是向立法者提供指导。</p><p>研究人员还建议对模型访问进行更严格的控制，包括关闭安全漏洞和限制对未来模型的访问。在内容方面，研究人员建议平台与人工智能供应商协调，以识别人工智能编写的内容，并要求所有内容由人类编写。最后，研究人员鼓励机构参与媒体扫盲活动，并提供以消费者为中心的人工智能工具。</p><p>虽然到目前为止还没有任何大型语言模型被用来传播虚假信息的记录，但像ChatGPT这样的模型的开放性导致一些人利用它在学校的作业和考试中作弊，例如。</p><p>报告的主要作者之一、安全与新兴技术中心的研究员Josh A. Goldstein表示，"我们不想等到这些模型被大规模部署用于影响行动时才开始考虑缓解措施。"</p> </div></body>
            </html>