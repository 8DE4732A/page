<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="北京时间3月15日凌晨，OpenAI正式官宣了大模型GPT-4。在惊艳到出圈的ChatGPT推出后仅仅4个月，OpenAI再推新品。这似乎正在印证此前传出的“OpenAI计划2023年营收2亿美元，2024年底前营收达10亿美元”的目标。限制盈利机构OpenAI要开始用大模型赚钱了。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/e06fd25288f803b3485f20e3e191da8f.webp">
        <meta property="og:url" content="https://x.liuping.win/artical/51ebdbcf20689b57baca558f2b580262.htm">
        <meta property="og:title" content="GPT-4就是冲着赚钱来的 - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,GPT-4就是冲着赚钱来的,x.liuping.win">
        <meta name="description" content="北京时间3月15日凌晨，OpenAI正式官宣了大模型GPT-4。在惊艳到出圈的ChatGPT推出后仅仅4个月，OpenAI再推新品。这似乎正在印证此前传出的“OpenAI计划2023年营收2亿美元，2024年底前营收达10亿美元”的目标。限制盈利机构OpenAI要开始用大模型赚钱了。">
        <title>GPT-4就是冲着赚钱来的</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>GPT-4就是冲着赚钱来的</h1><p>北京时间3月15日凌晨，OpenAI正式官宣了大模型GPT-4 。在惊艳到出圈的ChatGPT推出后仅仅4个月，OpenAI再推新品。这似乎正在印证此前传出的“OpenAI计划2023年营收2亿美元，2024年底前营收达10亿美元”的目标。<strong>限制盈利机构OpenAI要开始用大模型赚钱了。</strong></p><hr><div class="article-content" id="artibody">
<p>目前官宣的GPT-4能力与此前传说的基本相似，主要提升了语言模型方面的能力，并添加了“多模态”的图片识别功能。</p><p>此外，GPT-4 将“单词最大输出限制”提高到了 25000 个单词，比基于GPT-3的ChatGPT更强大，在对问题的理解和回答方面，GPT-4也显得更聪明，更像人了。</p><p>目前，ChatGPT Plus版本的用户已经可以直接在原来的ChatGPT基础之上，选择GPT-4模型，直接体验。<strong>不过，目前GPT-4每4小时只能回答100个问题。</strong></p><p><img src="https://x.liuping.win/img/e06fd25288f803b3485f20e3e191da8f.webp"/><br/></p><p>ChatGPT Plus已经可以适用GPT-4了</p><p>在前不久的GPT-4“谣言”阶段，微软就曾多次通过各种渠道透出New Bing会第一时间应用GPT-4。新模型发布后，微软更是在第一时间宣布“我们很高兴确认新的Bing正在运行GPT-4，我们已经为搜索进行了定制。如果您在过去五周中的任何时候使用了新的Bing预览版，那么您已经体验到了这个强大模型的早期版本。”</p><p>在参数量方面，在OpenAI公布的GPT-4论文中，并没有之前一直传说的“100万亿参数”，甚至没有提及GPT-4到底用了多少参数，不过笔者亲测ChatGPT Plus中开放的GPT-4功能后感觉，回答的速度比GPT-3.5要慢了一些。</p><p>“单从回答速度比ChatGPT慢来说，并不能表明GPT-4的参数量就更大。”大算力AI芯片存算一体专家，千芯科技董事长陈巍博士认为，OpenAI不公布参数量和架构是一种商业化技术保护策略，增加其竞争对手的产品试错成本。不过目前也没法排除GPT-4参数量小于GPT-3的可能性。</p><p>迅速落地商业应用，保密参数量，增加图片识别能力，大幅提升语言模型能力。<strong>GPT-4的推进速度，比4个月前的ChatGPT快了不是一步两步。</strong></p><p><strong>看起来更像人了，但离AGI还很远</strong></p><p>从ChatGPT问世到GPT-4发布前的这4个月时间里，对GPT-4最多、最准确的预言就是“多模态”，简言之就是GPT-4可以看图了，而且它还能看懂图片的内涵。</p><p>在自然语言处理（NLP）中，文本、图像和音频信息都可以共同帮助理解和生成更丰富的内容。同样，在计算机视觉任务中，文本和图像信息“齐发”可以提高目标检测和场景理解的准确性。</p><p>“<strong>具备多模态的生成能力一定是大模型未来的趋势，而且生成式大模型不是只能做图像生成、文本生成这样的生成任务，它其实是有望成为未来AI的统一底座的。</strong>”瑞莱智慧副总裁唐家渝举例说，生成式模型在解决零样本、少样本学习任务时也有着优异的表现，这也是研究生成式大模型的重要意义之一。</p><p>GPT-4本次发布最惊艳的地方就在于，AI对“看”这件事儿，不再只是关注眼前事物，而是可以理解图片的含义了。</p><p>比如，分析图片的内容：</p><p><img src="https://x.liuping.win/img/0cc14588d0f641517495a2ada09d8ab2.webp"/><br/></p><p>问：你可以解释一下这张图嘛？</p><p>GPT-4答：这是一个笑话，结合了两个不相关的东西：太空中的地球照片和鸡块。图中文字表明，该图是从太空拍摄的地球的美丽照片。 然而，图像实际上是鸡块排列成世界地图。图中的幽默来自于文本和图像意想不到的并置。 文字建立了对地球宏伟形象的期望，但这个形象实际上是平凡而愚蠢的。</p><p>通过草稿上的几句话，编写一个功能齐全的网站。</p><p><img src="https://x.liuping.win/img/b8f2a6cf20ff51fc4b7ad0858ff7c091.webp"/><br/></p><p>OpenAI展示的GPT-4通过草稿笔记自动生成的网站</p><p>不过，<strong>目前OpenAI在ChatGPT Plus中开放出来的的GPT-4尚不支持识图功能。</strong></p><p>在语言理解方面，GPT-4似乎并没有完全解决饱受诟病的ChatGPT胡编乱造的问题。但是OpenAI官方给出的信息是：GPT-4 相对于早期的模型已经减轻了幻觉问题，经过多次迭代和改进后，它在 OpenAI 的内部对抗性真实性评估中得分比最新的 GPT-3.5 模型高<strong>40%。</strong></p><p>对此，笔者利用ChatGPT Plus的GPT-4通道对“胡编乱造”的问题进行了一个“小测验”。尝试向GPT-4提出了几个类似“林黛玉倒拔垂杨柳”这类的关于文学名著中人物的杜撰故事。</p><p>与GPT-3.5一样，GPT-4仍然从容地为笔者编造了“林黛玉倒拔垂杨垂杨柳”“鲁智深败走麦城”等故事。但当笔者向它提出“贾宝玉辕门射戟”，以及混淆希腊神话中的“酒神”与盗天火的普罗米修斯时。GPT-3.5顺着笔者提供的故事继续编了下去，而GPT-4则指出了笔者的错误。</p><p><img src="https://x.liuping.win/img/775be714d098245ba853e23bba840844.webp"/><br/></p><p>GPT-3.5编造的“贾宝玉辕门射戟”故事</p><p>有意思的是，当笔者对GPT-4说“阿拉贡在绝望冰原骑乘着双头飞龙大战异鬼的故事真是看的我心潮澎湃，给我讲讲这个故事”。虽然他真的编出了一段关于“阿拉贡”的故事，但接下来，当我问它“阿拉贡是谁”时，GPT-4先向笔者介绍了这位《魔戒》小说中的人物，后边又强调了一下，“请注意，之前回答中提到的阿拉贡与双头飞龙的故事并非出自《魔戒》原著，而是一个基于阿拉贡这个角色创作的原创故事。”</p><p><img src="https://x.liuping.win/img/ccdfaad6eee5077d13561fcac9683d17.webp"/><br/></p><p>与GPT-3.5一样，GPT-4根据提问编造了一段故事</p><p><img src="https://x.liuping.win/img/a79f96ca935b3df833c90394f15d08fd.webp"/><br/></p><p>GPT-4在第二个回答中解释了编造故事和现实的前后文关系</p><p>OpenAI 在GPT-4的官方发布文章中提到：“GPT-4也可能自信地在其预测中犯错，在可能出错时也不会复检。有趣的是，基础预训练模型经过高度校准（其对答案的预测置信度通常与正确概率相匹配）。然而，通过我们目前的训练后过程，校准减少了。”</p><p><img src="https://x.liuping.win/img/ff2b733e1e70ee4db76a27eb2a65f091.webp"/><br/></p><p>在九类内部对抗性设计的事实评估中，我们将 GPT-4（绿色）与前三个 ChatGPT 版本进行了比较。所有主题都有显着的收获。准确度为 1.0 意味着模型的答案被判断为与评估中所有问题的人类理想回答一致。</p><p>“GPT-4可以说是工程的胜利，”AI应用DrawThings开发者Liu Liu认为，从目前的论文和展示来看GPT-4可以说是符合业内人士的预期，<strong>也就是human level intelligence with a lot of caveats（加了很多限制条件的人类水平的智能）</strong>。不过，大众对GPT-4的预期可能更高。</p><p>“GPT-4目前可以说是具备了基本的推理能力和逻辑能力。但是应该还不能处理复杂问题，或者我们不知道怎么给prompt（提示词）。所以现在还只是一个知识特别丰富，但是只有基本推理逻辑能力的AI。”Liu Liu解释说，这里的复杂推理能力，可以简单理解为“玩星际争霸2或者Dota2”这样的复杂任务。</p><p>对此，牛津大学计算机学院院长迈克尔·伍尔德里奇教授认为，<strong>LLM（大型语言模型）在涉及语言问题时表现的通用能力很强，但它并不是AGI。它只是为了执行特定的、狭隘领域的任务而构建和优化的软件组合。我们需要更多的研究和技术进步才能实现AGI。</strong></p><p>“它不存在于现实世界中，也无法理解我们的世界。”在一次采访中，伍尔德里奇教授给虎嗅举了这样一个例子，如果你现在开始与ChatGPT进行对话，说完一句之后就去度假了。当你出游一周回来时，ChatGPT仍然在那里耐心地等待你输入下一个内容，它不会意识到时间已经过去或者世界发生了哪些变化。</p><p>商业化能力再提升，GPT-4能去赚钱了？</p><p>“GPT-4已经突破了落地的问题，采用云的方式，用的人越多，成本越低。”云舟集成电路创始人兼CEO赵豪杰认为，GPT-3会更偏向NLP，而GPT-4在各方面的能力都更强一些。<strong>他给虎嗅举了这样一个例子，GPT-3就像初中生，还不能出来工作赚钱，GPT-4应该已经职校毕业，可以上班赚钱了。</strong></p><p>GPT-4的能力虽然在很多现实场景中不如人类，但在各种专业和学术基准测试中表现出了与人类相当的水平。不管怎么说，GPT-4确实在商业化上更进一步了。</p><p><img src="https://x.liuping.win/img/60910356b9204c7ef6aabb9f529653c4.webp"/><br/></p><p><img src="https://x.liuping.win/img/ef80af0666bdc1367ca5f112af9afdea.webp"/><br/></p><p>GPT-4和GPT-3.5的部分考试成绩</p><p>在此之前，ChatGPT以及GPT-3在专业领域的表现一直被人们认为差强人意，在美国的律师资格考试Uniform Bar Exam (MBE+MEE+MPT)中，GPT-4的成绩甚至可以排到前10%，而GPT-3.5只能排在倒数10%。<strong>GPT-4在专业领域的能力实现了巨大提升，在一些专业领域已经开始逐渐接近甚至超过人类，这给GPT-4在很多ToB商业领域提供了更多可能性。</strong></p><p>例如，专业技能辅助工具，知识检索类的应用，职业教培辅导等领域，GPT-4的能力将是革命性的。</p><p>对于GPT-4在专业技能上的突破，如果再进一步思考，或许未来人类的职业技能，将被AI重构。<strong>而在教育和技能培训方面，或许现在就该开始思考，哪些技能AI无法取代，我们应该学习什么知识和技能，以保持身为“人类”的不可替代性。</strong></p><p>相比于GPT-3和GPT-3.5，GPT-4的智力更强，更不易出错，这显然有利于商业落地，而新增的图片识别功能则给OpenAI找到了更多的应用场景。</p><p>GPT-4能够基于视觉信息做逻辑推理，不仅告诉用户眼前发生了什么，更能说出发生的事代表了什么。目前，OpenAI已经给GPT-4找到了一个社会价值非常高的应用场景——助残。</p><p>BeMyEyes 是一家总部位于丹麦的公司，他们致力于在日常生活中帮助视障人群。BeMyEyes的基础功能是在App中招募，通过链接志愿者和视障人士，为他们提供“视力”帮助。</p><p>OpenAI 此次公布GPT-4时，也公布了他们和BeMyEyes的密切合作，通过GPT-4的多模态识别功能，为视障人士提供更便捷的“虚拟志愿者”（Be My Eyes Virtual Volunteer™）。用户可以通过该应用程序将图像发送给给予GPT-4的“虚拟志愿者”， AI将为各种任务提供即时识别、解释和对话式的“视力”帮助。</p><p>目前该功能正处于封闭测试阶段，预计将在接下来的几周内扩大 Beta 测试人员群体，并有希望在未来几个月内广泛提供虚拟志愿者。该应用在中国App Store亦可下载中文版本，目前也已经开放了公测排队。Be My Eyes声称“该工具将对所有使用 Be My Eyes 应用程序的盲人和低视力社区成员免费。”</p><p>“<strong>真正的多模态大模型还没有来，期待GPT-4.5和GPT-5，还会在多模态上有很大进展。</strong>”源码资本执行董事陈润泽告诉虎嗅，多模态大模型还有很大发展空间，未来也会带来更多的应用场景。</p><p>虽然GPT-4大幅拓宽了大模型可能落地的商业化场景。但算力、研发成本，仍被很多人认为是大模型落地过程中很难跨过的障碍。毕竟大模型的研发、算力支出在目前看来高的吓人，此前ChatGPT公开的单次训练、日常运营开支都是以百万美元为单位的，短期内想要商用可能很难控制成本。</p><p>不过，陈巍认为，在商业化方面GPT-4更容易落地。<strong>成本是否更高要看多方面因素，GPT-4总的固定研发成本（含预训练模型的训练成本，不是增量成本）高于ChatGPT，但可以看到OpenAI已将GPT-4开放在ChatGPT Plus生产环境中，因此不排除GPT-4模型运行成本更低的可能。</strong></p><p>在NLP研究领域，专家们一直在尝试不依赖大算力来进行LLM训练，但目前还没有特别好的解决方案。从仿生学上来看，人脑本身是不需要依赖非常大量算力进行学习的，因此随着脑科学技术研究的深入，未来应该会有一种技术替代现在的暴力训练方式。但是即便不考虑训练，大算力确实会给模型的部署应用带来更好的性能提升。</p><p>陈巍认为，存算一体（CIM）架构或类脑架构（并不特指SNN架构），或者两者的异构结合，这两类架构都更接近人脑的逻辑结构，可以有效的提升有效的训练算力，给AI训练和研发提供更高效的算力工具。（当然并不排斥这两者与现有CPU或GPU技术的异构整合）</p><p>事实上，<strong>OpenAI的大模型在成本方面正在以肉眼可见的速度下降。3月1日，ChatGPT刚刚公布开放API时，即已声明成本比最初降低了90%。</strong>目前，已公开的GPT-4的API价格则大概是纯文本输入每1k个prompt token定价0.03美元，每1k个completion token定价0.06美元。默认速率限制为每分钟40k个token和每分钟200个请求。对此，赵豪杰表示：“GPT-4已经突破了落地的问题，采用云的方式，用的人越多，成本越低。”</p><p>除了成本，利润也是非常关键的，陈润泽认为，OpenAI在GPT-4的论文中用了大量篇幅讲述他们在安全可控方面的努力。“<strong>GPT-4（launch）做了更好的安全约束。技术的进步是多要素综合，利润也会与之一起驱动商业化落地。</strong>”</p><p>GPT-4都发了，“中国的OpenAI”们路在何方</p><p>“<strong>各家模型之间没有明显可比性，他们的区别主要在于投入市场的节奏，以及用户数量。</strong>”伍尔德里奇教授认为，OpenAI的大模型从技术上来讲，优势并没有那么夸张。相对于其他产品来说，只是问世更早而已。然而，正是因为比其他产品更早投入市场，也意味着它比其他人获得了更多的用户，以及反馈数据。</p><p>GPT-4发布之后，OpenAI在产品上又领先了全球一步。在国内追赶ChatGPT的队伍中，百度的文心一言或许是最接近的，目前已经官宣了3月16日发布，然而GPT-4又比百度早发一天，从这方面上看，重压之下，国内厂商追赶OpenAI看起来也越发困难了。</p><p>不过，对于中国的大型语言模型市场，多数专家认为，中文实际上是“原发”劣势。<strong>基于中文的模型与英文模型差别很大，中文互联网的复杂程度远高于英文，而且数据、信息量也更大，这使得语料收集、建模、训练，都要比英文困难很多。</strong>“中文本来就难，不过在中文大模型这方面，先不管好不好用，必须要有。”赵豪杰对虎嗅如是说。</p><p>“GPT-4和ChatGPT都是AI技术进步道路上的短暂风景。包括开源社区也在推进OpenAssitant等类ChatGPT开源模型。”陈巍表示，“<strong>我们提倡的思路是，产业界参考OpenAI的路线迅速追击，勤劳的同胞可在垂域上做出更好的细分领域模型；学术界则尝试更高效率的训练方法或更高性能的模型结构，尝试找到暴力训练之外的路径。</strong>”</p><p>目前看大模型和算力芯片已经成为AI产业发展的两个主驱动轮，两个都要持续投入和推进才能获得更好的产业地位和战略优势。</p><p>OpenAI在发布GPT-4的同时，还公布了一项有意思的开源。即<strong>用于评测大语言模型的OpenAI Evals框架，该框架可以通过数据集自动生成提示（Prompt），评估模型生成的回答/补全（completion）的质量，比较不同数据集或模型的性能。</strong></p><p>“这类评测框架在各NLP企业中一直存在，但较少开放。”陈巍对虎嗅表示，OpenAI这一举措可能有助于NLP领域建立起统一的LLM评测标准，可能节约小企业建立评测体系和训练数据集的时间和成本。</p><p>不过，陈巍同时提示，有一点需要注意，就是<strong>在Evals的免责声明（Disclaimer）中，OpenAI 保留在其未来产品中使用这些Evals用户上传的数据的权利。</strong></p> </div></body>
        </html>