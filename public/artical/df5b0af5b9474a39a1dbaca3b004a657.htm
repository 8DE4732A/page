<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="这是首个基于Transformer的驾驶行为描述框架，可以感知和预测驾驶行为。也就是说，输入车辆视频后，这个算法可以判断车辆行为并告诉你：车在做什么，为什么要这么做。还在玩ChatGPT？已经有自动驾驶算法能告诉你“我在干嘛”。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/7dc37e1ba9a72277a4a5bd4dc57122bd.png">
        <meta property="og:url" content="https://x.liuping.win/artical/df5b0af5b9474a39a1dbaca3b004a657.htm">
        <meta property="og:title" content="他们用ChatGPT方式搞自动驾驶 论文入选了ICRA 2023 - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,他们用ChatGPT方式搞自动驾驶 论文入选了ICRA 2023,x.liuping.win">
        <meta name="description" content="这是首个基于Transformer的驾驶行为描述框架，可以感知和预测驾驶行为。也就是说，输入车辆视频后，这个算法可以判断车辆行为并告诉你：车在做什么，为什么要这么做。还在玩ChatGPT？已经有自动驾驶算法能告诉你“我在干嘛”。">
        <title>他们用ChatGPT方式搞自动驾驶 论文入选了ICRA 2023</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>他们用ChatGPT方式搞自动驾驶 论文入选了ICRA 2023</h1><p>这是首个基于Transformer的驾驶行为描述框架，可以感知和预测驾驶行为。也就是说，输入车辆视频后，这个算法可以判断车辆行为并告诉你：车在做什么，为什么要这么做。还在玩ChatGPT？已经有自动驾驶算法能告诉你“我在干嘛”。</p><hr><div class="article-content" id="artibody">
<p>基于视觉和神经网络的自动驾驶算法，虽然能通过传感器数据，以及学习人类的驾驶行为，自主决策并控制车辆。</p><p><img alt="" id="0" src="https://x.liuping.win/img/7dc37e1ba9a72277a4a5bd4dc57122bd.png"/><br/></p><p>但是，算法基于什么做出的决策？特别是出现故障，也就是决策错误的时候，<span style="">算法是怎么想的？</span>这些一直被称为自动驾驶算法里的“黑匣子”，让算法缺乏<span style="">透明度</span>和<span style="">可解释性</span>。</p><p>不过，现在有这么一个模型，既能预测车辆控制行为，还能自己解释“我停车是因为红灯亮了，并且有行人在过马路”。</p><p>模型论文入选<span style="">ICRA 2023</span>，相关模型已开源。</p><p>那么，是一个什么样的算法？</p><p>ADAPT：驾驶行为感知说明大模型</p><p>这是一种叫ADAPT（Action-aware Driving Caption Transformer）的端到端算法，也是目前第一个基于Transformer的驾驶行为描述框架，可以感知和预测驾驶行为，并且输出自然语言叙述和推理。</p><p><img alt="" id="1" src="https://x.liuping.win/img/bbad191f56f24936d54ee7b81040d31c.png"/><br/></p><p>直白一点说，输入车辆视频后，这个算法可以判断车辆行为并告诉你：车在做什么，为什么要这么做。</p><p><img alt="" id="2" src="https://x.liuping.win/img/249d5e7e4cadc4dd84f63f26c5b45b18.png"/><br/></p><p>在论文作者提供的测试视频里，这个算法最终上车的效果是这样的。（红色字是车辆行为，蓝色字是解释）</p><p>“车在向前开。因为路上没有车。”</p><p><img alt="" id="3" src="https://x.liuping.win/img/aabcdeab2c2c92cdc1f449ae2a736ec3.png"/><br/></p><p>驾驶行为变化后，算法也能及时感知：</p><p>“车靠左边停下了。因为要停车。”</p><p><img alt="" id="4" src="https://x.liuping.win/img/cda57174e120794510a02dcd23e9ce43.png"/><br/></p><p>“车开始移动并且靠右行驶。因为路左边停着车。”</p><p><img alt="" id="5" src="https://x.liuping.win/img/6346b7ec901e1ab98409e6f34870bd82.png"/><br/></p><p>算法不仅能识别路口，也能识别骑着车的人。</p><p>“车在十字路口停下了。因为要避开街上骑着自行车的人，”</p><p><img alt="" id="6" src="https://x.liuping.win/img/1c397be9f89d0271a3cca6373ca06abd.png"/><br/></p><p>这是怎么实现的？</p><p>多任务框架下的联合训练</p><p>ADAPT框架可以分为两个部分：<span style="">车辆行为描述</span>（DCG，Driving Caption Generation）和<span style="">车辆控制信号预测</span>（CSP，Control Signal Prediction）。</p><p><img alt="" id="7" src="https://x.liuping.win/img/4fe7c6213d43ef4b9789c7f6b1ffeaf7.png"/><br/></p><p>首先，传感器端输入视频，Video Swin Transformer对车辆视频进行编码，得到的视频特征会输入进各任务模块里。</p><p>在DCG模块，算法利用Vision-Language Transformer生成两个自然语句，也就是上文中提到的<span style="">车辆行为描述</span>和<span style="">原因解释</span>。</p><p>相同的视频特征也会输入进CSP模块（类似一般基于视觉的自动驾驶系统），输出车辆实际的控制信号序列，并利用Motion Transformer输出模型预测的控制信号，比如速度、方向和加速度。</p><p>在单个网络中，作者利用车辆实际的控制信号序列和模型预测的控制信号序列，两者的均方误差作为CSP模块的损失函数。</p><p><img alt="" id="8" src="https://x.liuping.win/img/c19b6de7a600a7484429b24d44261516.png"/><br/></p><p>而在多任务框架下，通过联合训练DCG和CSP，可以减少车辆决策和文本描述之间的差异，提高控制信号预测的准确率。</p><p>论文里，作者们在包含控制信号和车辆视频的大规模数据集<span style="">BDD-X</span>上，利用机器评测和人工评测验证了ADAPT的有效性。</p><p>机器评测方面，使用的是BLEU4、METEOR、ROUGE-L和CIDEr（对应缩写分别为B4、M、R、C）等多种语言任务常用的指标。</p><p><img alt="" id="9" src="https://x.liuping.win/img/7f4fd1154abb5584c99ee14af8811247.png"/><br/></p><p>最终显示ADAPT达到了<span style="">当前最优</span>（State-of-the-Art）的结果，ADAPT在动作描述方面比原有先进方法CIDEr高出31.7，在原因解释方面高33.1。</p><p><img alt="" id="10" src="https://x.liuping.win/img/ed8bd74948ca41e42ebec756060308bc.png"/><br/></p><p>人工评测分为动作描述、原因解释和全句三个部分。通过人工判断，ADAPT在这三部分的准确性分别达到了90%，90.3%和82.7%，证明了ADAPT的有效性。</p><p><img alt="" id="11" src="https://x.liuping.win/img/ea49b9e7a859a0b479c0f4fe92855ae8.png"/><br/></p><p>在可视化结果里，也能看出ADAPT可以准确识别车辆行为以及决策原因。并且在黑夜、阴雨天等场景下，ADAPT也能保证准确度；即使有雨刷器干扰，ADAPT也可以识别道路上的停止标识。</p><p>为什么需要ADAPT？</p><p>自动驾驶行为的可解释性</p><p>在基于视觉的自动驾驶算法里，比较常见的解释图有视觉注意图（Attention Map），或者成本量图（Cost Volume），但不熟悉自动驾驶算法的人容易对这些图造成误解。</p><p><img alt="上：视觉注意图；下：成本量图" id="12" src="https://x.liuping.win/img/c319c7cf4b58aa36887ed124c00e3fbf.png"/><br/>上：视觉注意图；下：成本量图</p><p>因此，ADAPT这种能够生成自然语言、“说人话”的算法，能够帮助用户更好地理解自动驾驶算法在做什么、为什么要这么做，同时还能让用户更信任自动驾驶技术。</p><p>而对于算法工程师和研究人员来说，当发生极端情况时、或者发生故障（比如判断错误）时，ADAPT可以帮助他们获得更多信息，进而改进算法。</p><p><img alt="" id="13" src="https://x.liuping.win/img/95a6663bb308ac44a3c3891dbd91e9f7.png"/><br/></p><p>作者们将进一步研究如何在模拟器和实际车辆上如何部署ADAPT，以及如何利用文本转语音技术，让生成的句子转化为语音，帮助普通乘客，特别是视力障碍乘客使用。</p> </div></body>
        </html>