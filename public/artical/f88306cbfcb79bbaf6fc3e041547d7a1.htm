<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="不做大模型，就没有算力用。这是ChatGPT点燃AI风口后，国内某top3高校AI实验室的残酷现状。同一个实验室里，非大模型团队6人用4块3090卡，比起同实验室的大模型团队10个人用10块A800卡，本就已经不算富裕。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/c77ab9baba1193765ee38548de71ac5d.webp">
        <meta property="og:url" content="https://x.liuping.win/artical/f88306cbfcb79bbaf6fc3e041547d7a1.htm">
        <meta property="og:title" content="ChatGPT造孽 中国高校因它算力荒 - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,ChatGPT造孽 中国高校因它算力荒,x.liuping.win">
        <meta name="description" content="不做大模型，就没有算力用。这是ChatGPT点燃AI风口后，国内某top3高校AI实验室的残酷现状。同一个实验室里，非大模型团队6人用4块3090卡，比起同实验室的大模型团队10个人用10块A800卡，本就已经不算富裕。">
        <title>ChatGPT造孽 中国高校因它算力荒</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>ChatGPT造孽 中国高校因它算力荒</h1><p><strong>不做大模型，就没有算力用。</strong>这是ChatGPT点燃AI风口后，国内某top3高校AI实验室的残酷现状。同一个实验室里，非大模型团队6人用4块3090卡，比起同实验室的大模型团队10个人用10块A800卡，本就已经不算富裕。</p><hr><div class="article-content" id="artibody">
<p>现在，<strong>校企合作</strong>也更偏爱大模型。去年11月ChatGPT发布后，与非大模型团队合作的企业骤减，近期找上门的，也是张口就问：</p><p><strong>“你们做大模型不？”</strong></p><p>做，有高校和企业的通力支持；不做？那就只能眼睁睁看着算力花落别家。</p><p>哪怕某量化私募基金的有10000张A100卡，还对高校研究团队开放申请，也不见得能落一张到你头上。</p><p><img src="https://x.liuping.win/img/c77ab9baba1193765ee38548de71ac5d.webp"/><br/></p><p>“要是我们组能分到一些就好了。”看到这条微博，非大模型团队带队的数据科学方向博士小哥羡慕不已，因为缺算力，他都愁得快仰天长啸了：我们也值得投资啊！！！</p><p>现在，大伙争先恐后扑向ChatGPT背后GPT-3.5般的各种大模型，算力流向亦然。</p><p>其他AI领域本就不足的算力<strong>更荒了</strong>，尤其是国内学界手里的算力分配下来，贫富差距肉眼可见。</p><p>一整个实验室就4块3090卡</p><p>巨大规模算力以月为单位的租用成本，对研究团队来说不是小数目。大模型正当其道，学界研究大模型的实验室或团队拥有算力资源的<strong>优先分配权</strong>。</p><p>就拿小哥在学校的亲身体验来说，在他们研究室，大模型小组10个人有10块A800卡可用，而另一个研究传统机器学习方向的实验室，<strong>整个实验室只有4块3090卡</strong>。</p><p>拥抱主流趋势是一重原因，另一重原因是实验室需要运转和维护的经费，获得拨款的一种形式是申请国家项目，但必要步骤是提供论文成果。</p><p>双重原因下，本就不多的算力资源，<strong>不得不优先分配给大模型</strong>这样热门且相对容易出成果的研究。哪怕对学界来说，训一个大模型其实练不太动——因为数据、算力和资金都有些捉襟见肘。</p><p>为了获得更多的资源，有的非大模型实验室甚至额外专门成立研究大模型的团队。</p><p>当然，想要获得资金和资源，<strong>校企合作</strong>也是不可或缺的一种方式。</p><p>这种推动产研融合的重要支撑形式持续已久，2020年，KDD中校企合作论文占比超过50%，这个比例在ICCV中达到45%。</p><p>举例来说，2021年，清华大学KEG、PACMAN（并行与分布式计算机系统）、NLP等实验室着手推进训练千亿参数的稠密模型，但团队用于训练模型的计算资源并不充足。最终，校外企业智谱AI租用了近百台A100的服务器，<strong>免费提供所需算力</strong>，这才有了双语预训练语言大模型GLM-130B的诞生。</p><p><strong><img src="https://x.liuping.win/img/990f53405c970fb919b00605769ce4ba.webp"/><br/></strong></p><p>GLM-130B的任务表现</p><p>但在众人争先恐后扑向GPT-3.5般大模型的当下，非大模型团队开始<strong>不太好谈</strong>这类合作了。</p><p>去年11月ChatGPT发布后，与小哥所在团队洽谈校企合作事宜的公司数量急剧减少。在其他高校，AI领域的非大模型团队也总是面临企业询问，“要不要/会不会做大模型”。</p><p>本就稀缺的算力，在学界有成为追逐热点的砝码的倾向，算力资源分配的<strong>马太效应</strong>由此逐渐扩大，带给学术研究很大困扰。</p><p>ChatGPT加剧算力分配贫富分化</p><p>算力是AI飞速发展必不可少的指标，2018年，OpenAI发布的报告中点出一个<strong>算力趋势</strong>：</p><p>自2012年以来，AI训练任务所运用的算力每3.43个月就会翻倍。到2018年，AI算力需求增长了30万倍。</p><p><img src="https://x.liuping.win/img/2170092d62ac2835c977b16e175d3567.webp"/><br/></p><p><strong>产学研对算力需求暴增，我们能提供的算力有多少？</strong></p><p>据中国算力集团统计，截至2022年6月底，我国数据中心机架使用总规模超过590万标准机架，服务器规模约2000万台，算力总规模排名<strong>全球第2</strong>。</p><p>这个排名还算不错，但摊开来看仍旧远远不够，毕竟放眼全球，<strong>没有哪个国家不是嗷嗷待哺，等着更多的算力资源“投喂”</strong>。</p><p>再退一步讲，买得起显卡，拥有的算力上去了，电费也是天文数字。</p><p>况且我国还有特殊情况——</p><p>开放原子开源基金会业务发展部部长<strong>朱其罡</strong>在本月举办的CCF YOCSEF上发言阐述现状称，超算领域的核心技术，一个是IBM LSF超算系统，一个是开源系统。目前，国内多数超算中心都基于开源系统做封装，但这个版本调度资源的效率和能力都有很大的提升空间。</p><p>以及，因为众所周知的原因，A100、H100这俩目前性能最强的GPU，还没找到可规模替代的方案。</p><p><strong><img src="https://x.liuping.win/img/ad4f25db505689f8ee9e1af0ee757e9c.webp"/><br/></strong></p><p>英伟达A100显卡</p><p>综上，算力不够已是积弊，但ChatGPT时代，算力需求剧烈扩张，除了大量训练算力，大量<strong>推理算力</strong>也需要支撑。</p><p>所以现在的情况是，因为ChatGPT显示出大模型的推理能力，训练和研究大模型的算力需求增加；同时因为大模型热度爆棚，蜂拥至大模型的算力资源也增加。</p><p>分配给大模型领域的算力资源丰富起来，其他AI领域缺衣少食的情况逐渐加剧，研发能力受到掣肘。</p><p>可以说，ChatGPT成为如今的AI届白月光后，<strong>加剧了算力分配的贫富分化</strong>。</p><p>这般“富”甲一方的大模型，是不是AI研究路径上最好的？还没人能够回答。</p><p>但值得引起注意和重视的是，GPT系列为首的大模型<strong>不应该吸引全部目光</strong>，整个AI领域还有各种各样的研究方向，还有更加细分的垂直领域，以及带来更多生产力的模型和产品。</p><p>当ChatGPT的热度趋于平缓，学界的算力资源分配差距会缩小吗？</p><p>所有非大模型方向的实验室和团队，恐怕都在期待之中。</p> </div></body>
        </html>