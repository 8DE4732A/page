<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="在ChatGPT引爆科技领域之后，人们一直在讨论AI“下一步”的发展会是什么，很多学者都提到了多模态，我们并没有等太久。今天凌晨，OpenAI发布了多模态预训练大模型GPT-4。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/5d624213e2f764c44093530a6826adf2.webp">
        <meta property="og:url" content="https://x.liuping.win/artical/515ae8d44ef6c2a9cac9bfcf28074179.htm">
        <meta property="og:title" content="GPT-4震撼发布：多模态大模型 直接升级ChatGPT、必应，开放API - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,GPT-4震撼发布：多模态大模型 直接升级ChatGPT、必应，开放API,x.liuping.win">
        <meta name="description" content="在ChatGPT引爆科技领域之后，人们一直在讨论AI“下一步”的发展会是什么，很多学者都提到了多模态，我们并没有等太久。今天凌晨，OpenAI发布了多模态预训练大模型GPT-4。">
        <title>GPT-4震撼发布：多模态大模型 直接升级ChatGPT、必应，开放API</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>GPT-4震撼发布：多模态大模型 直接升级ChatGPT、必应，开放API</h1><p>在 ChatGPT 引爆科技领域之后，人们一直在讨论 AI“下一步”的发展会是什么，很多学者都提到了多模态，我们并没有等太久。今天凌晨，OpenAI 发布了多模态预训练大模型 GPT-4。</p><hr><div class="article-content" id="artibody">
<p><img src="https://x.liuping.win/img/5d624213e2f764c44093530a6826adf2.webp"/><br/></p><p><strong>GPT-4 实现了以下几个方面的飞跃式提升：强大的识图能力；文字输入限制提升至 2.5 万字；回答准确性显著提高；能够生成歌词、创意文本，实现风格变化。</strong></p><p><img src="https://x.liuping.win/img/7bf99e29609c49e2e62499c9ff3350a3.gif"/><br/></p><p>“GPT-4 是世界第一款高体验，强能力的先进AI系统，我们希望很快把它推向所有人，”OpenAI 工程师在介绍视频里说道。</p><p>似乎是想一口气终结这场游戏，OpenAI 既发布了论文（更像是技术报告）、 System Card，把 ChatGPT 直接升级成了 GPT-4 版的，也开放了 GPT-4 的 API。</p><p>另外，微软营销主管在 GPT-4 发布后第一时间表示：“如果你在过去六周内的任何时候使用过新的 Bing 预览版，你就已经提前了解了 OpenAI 最新模型的强大功能。”是的，微软的新必应早就已经用上了GPT-4。</p><p><img src="https://x.liuping.win/img/1a96d51854dd3876199cf523f540e8d4.webp"/><br/></p><p>接下来，就让我们细细品味这场震撼发布。</p><p><strong>GPT-4：我 SAT 考 710，也能当律师</strong></p><p>GPT-4 是一个大型多模态模型，能接受图像和文本输入，再输出正确的文本回复。实验表明，GPT-4 在各种专业测试和学术 基准上的表现与人类水平相当。例如，它通过了模拟律师考试，且分数在应试者的前 10% 左右；相比之下，GPT-3.5 的得分在倒数 10% 左右。</p><p>OpenAI 花了 6 个月的时间使用对抗性测试程序和 ChatGPT 的经验教训对 GPT-4 进行迭代调整 ，从而在真实性、可控性等方面取得了有史以来最好的结果。</p><p>在过去的两年里，OpenAI 重建了整个 深度学习堆栈，并与 Azure 一起为其工作负载从头开始设计了一台超级计算机。一年前，OpenAI 在训练 GPT-3.5 时第一次尝试运行了该超算系统，之后他们又陆续发现并修复了一些错误，改进了其理论基础。这些改进的结果是 GPT-4 的训练运行获得了前所未有的稳定，以至于 OpenAI 能够提前准确预测 GPT-4 的训练性能，它也是第一个实现这一点的大模型。OpenAI 表示他们将继续专注于可靠的扩展，进一步完善方法，以帮助其实现更强大的提前预测性能和 规划未来的能力，这对安全至关重要。</p><p>OpenAI 正在通过 ChatGPT 和 API（有候补名单）发布 GPT-4 的文本输入功能。图像输入功能方面，为了获得更广泛的可用性，OpenAI 正在与其他公司展开合作。</p><p>OpenAI 今天还开源了 OpenAI Evals，这是其用于自动评估 AI 模型性能的框架。OpenAI 表示此举是为了让所有人都可以指出其模型中的缺点，以帮助 OpenAI 进一步改进模型。</p><p>有趣的是，GPT-3.5 和 GPT-4 之间的区别很微妙。当任务的复杂性达到足够的阈值时，差异就会出现 ——GPT-4 比 GPT-3.5 更可靠、更有创意，并且能够处理更细微的指令。为了了解这两个模型之间的差异，OpenAI 在各种 基准和一些为人类设计的模拟考试上进行了实验。</p><p><img src="https://x.liuping.win/img/ae434bedb3ec06033ed04f5d0c445024.webp"/><br/></p><p><img src="https://x.liuping.win/img/b9ff058a96b2416a82caddf5cba795f4.webp"/><br/></p><p>OpenAI 还在为 机器学习模型设计的传统 基准上评估了 GPT-4。GPT-4 大大优于现有的大型 语言模型，以及大多数 SOTA 模型：</p><p><img src="https://x.liuping.win/img/81247d5bff2e77e97ac8bfa45d2252ed.webp"/><br/></p><p>许多现有的 机器学习 基准测试都是用英语编写的。为了初步了解 GPT-4 在其他语言上的能力，研究团队使用 Azure Translate 将 MMLU 基准 —— 一套涵盖 57 个主题的 14000 个多项选择题 —— 翻译成多种语言。在测试的 26 种语言的 24 种中，GPT-4 优于 GPT-3.5 和其他大 语言模型（Chinchilla、PaLM）的英语语言性能：</p><p><img src="https://x.liuping.win/img/02d1c281ee3a6ac02e4531d206c6ecaa.webp"/><br/></p><p>就像许多使用 ChatGPT 的公司一样，OpenAI 表示他们内部也在使用 GPT-4，因此 OpenAI 也在关注大型 语言模型在内容生成、销售和编程等方面的应用效果。OpenAI 还使用 GPT-4 辅助人们评估 AI 输出，这也是 OpenAI 对其策略的第二阶段。OpenAI 既是 GPT-4 的开发者，也是使用者。</p><p><strong>GPT-4：我能玩梗图</strong></p><p>GPT-4 可以接受文本和图像形式的 prompt，新能力与纯文本设置并行，允许用户指定任何视觉或语言任务。</p><p>具体来说，它在人类给定由散布的文本和图像组成的输入的情况下生成相应的文本输出（自然语言、代码等）。在一系列领域 —— 包括带有文本和照片的文档、图表或屏幕截图上 ——GPT-4 展示了与纯文本输入类似的功能。此外，它还可以通过为纯文本 语言模型开发的测试时间技术得到增强，包括少样本和思维链 prompt。</p><p>比如给 GPT-4 一个长相奇怪的充电器的图片，问为什么这很可笑？</p><p><img src="https://x.liuping.win/img/2bb57e0a6411ab00f6912a89267829c0.webp"/><br/></p><p>GPT-4 回答道，VGA 线充 iPhone。</p><p><img src="https://x.liuping.win/img/5ac4b894c5a6f506277c5a15093f2ba3.webp"/><br/></p><p>格鲁吉亚和西亚的人均每日肉类消费，算平均数：</p><p><img src="https://x.liuping.win/img/83c527869887b6962c2710e7ea05b1bd.webp"/><br/></p><p>看起来，现在的 GPT 已经不会在计算上胡言乱语了：</p><p><img src="https://x.liuping.win/img/18ecadd9f0ef7d58de0e2025eeda28ed.webp"/><br/></p><p>还是太简单，那直接让它做题，还是个物理题：</p><p><img src="https://x.liuping.win/img/fe69240e66a8e6f03c0e63d25440d31f.webp"/><br/></p><p>GPT-4 看懂了法语题目，并完整解答：</p><p><img src="https://x.liuping.win/img/d99b3906861738cd5a1d8d3b04d7ce29.webp"/><br/></p><p>GPT-4 可以理解一张照片里“有什么不对劲的地方”：</p><p><img src="https://x.liuping.win/img/a91ecca8e38991ad2dc9619cb49604cf.webp"/><br/></p><p>GPT-4 还可以量子速读看论文，如果你给它 InstructGPT 的论文，让它总结摘要，就会变成这样：</p><p><img src="https://x.liuping.win/img/9e2458980e33016ad20626a6e9ac6c59.webp"/><br/></p><p><img src="https://x.liuping.win/img/8849f72c3d6a9a12133929f70df2a6aa.webp"/><br/></p><p>如果你对论文里的某一个图感兴趣呢？GPT-4 也可以解释一下：</p><p><img lazyload="https://x0.ifengimg.com/res/2023/4FEAF362B990ADA876BBDEE894FCF40F3A66B34A_size634_w1080_h765.png" src="https://x.liuping.win/img/494d00787c4d406f9746796f1053100c.png"/><br/></p><p>接着来，问 GPT-4 梗图是什么意思：</p><p><img lazyload="https://x0.ifengimg.com/res/2023/BFAA14DC3EF147F24D5F784034942E0E05AEBC20_size466_w736_h725.png" src="https://x.liuping.win/img/c731d0d782cd71bc1e3f1b87602bbfa6.png"/><br/></p><p>它给出了详细的回答：</p><p><img lazyload="https://x0.ifengimg.com/res/2023/BE50A36F4DB438AC01F592C79A5CE19EE7F335CC_size312_w1080_h304.png" src="https://x.liuping.win/img/2bce9c43f087b76d219da4bea67fdbad.png"/><br/></p><p>那么漫画呢？</p><p><img lazyload="https://x0.ifengimg.com/res/2023/CCD8835A32CE4B3855741088C14C3A470C451594_size246_w888_h659.png" src="https://x.liuping.win/img/444237f4f6a8f2a00e8caddf943dba13.png"/><br/></p><p>让 GPT-4 解释为什么要给 神经网络加层数，似乎有一点加倍的幽默感。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/3987978A7EA5074CE63577D7DF604A8A7DA41AF9_size582_w1080_h577.png" src="https://x.liuping.win/img/5ff9475ee93eff6d84b7d18eae1b3ab4.png"/><br/></p><p>不过 OpenAI 在这里说了，图像输入是研究预览，仍不公开。</p><p>研究人员用学术的 Benchmark 视角来解读 GPT-4 的看图能力，然而这已经不够了，他们还能不断发现该模型可以令人兴奋地处理新任务 —— 现在的矛盾是 AI 的能力和人类想象力之间的矛盾。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/6EE611380154A93416BF0D2B97AF3A255D21C499_size50_w1080_h419.png" src="https://x.liuping.win/img/e62a88aa8f3e70cac24e5b0d0c2480cc.png"/><br/></p><p>看到这里，应该有研究人员感叹：CV 不存在了。</p><p><strong>可控性</strong></p><p>与具有固定冗长、平静语气和风格的经典 ChatGPT 个性不同，开发人员（以及 ChatGPT 用户）现在可以通过在“系统”消息中描述这些方向来规定他们的 AI 的风格和任务。</p><p>系统消息允许 API 用户在一定范围内定制化实现不同的用户体验。OpenAI 知道你们在让 ChatGPT 玩 Cosplay，也鼓励你们这样做。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/2BEEC0FB2DC31EAF23CD85F9110E0E5050759178_size337_w1080_h748.png" src="https://x.liuping.win/img/ce137be5546f4a18b979437bb4e2936c.png"/><br/></p><p><strong>局限性</strong></p><p>尽管功能已经非常强大，但 GPT-4 仍与早期的 GPT 模型具有相似的局限性，其中最重要的一点是它仍然不完全可靠。OpenAI 表示，GPT-4 仍然会产生幻觉、生成错误答案，并出现推理错误。</p><p>目前，使用 语言模型应谨慎审查输出内容，必要时使用与特定用例的需求相匹配的确切协议（例如人工审查、附加上下文或完全避免使用） 。</p><p>总的来说，GPT-4 相对于以前的模型（经过多次迭代和改进）已经显著减轻了幻觉问题。在 OpenAI 的内部对抗性真实性评估中，GPT-4 的得分比最新的 GPT-3.5 模型高 40%：</p><p><img lazyload="https://x0.ifengimg.com/res/2023/DD17DC7AA33177FACB02045CC7ECAB136C34628F_size189_w1080_h677.png" src="https://x.liuping.win/img/75d8ca5ec60894005ec1eefa35deeaab.png"/><br/></p><p>GPT-4 在 TruthfulQA 等外部 基准测试方面也取得了进展，OpenAI 测试了模型将事实与错误陈述的对抗性选择区分开的能力，结果如下图所示。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/6C4B89C09B5338884B2268A59AAF3D972F9ECC7C_size99_w1080_h620.png" src="https://x.liuping.win/img/6a8b6b093303fc11cb734e77224fd39f.png"/><br/></p><p>实验结果表明，GPT-4 基本模型在此任务上仅比 GPT-3.5 略好；然而，在经过 RLHF 后训练之后，二者的差距就很大了。以下是 GPT-4 的测试示例 —— 并不是所有时候它都能做出正确的选择。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/7803D07E34EBBCA6E965EC1A3633CD4B3FE30EEA_size77_w1080_h178.png" src="https://x.liuping.win/img/0d5d609a9a296b99bb1a0d737e405796.png"/><br/></p><p>该模型在其输出中可能会有各种偏见，OpenAI 在这些方面已经取得了进展，目标是使建立的 人工智能系统具有合理的默认行为，以反映广泛的用户价值观。</p><p>GPT-4 通常缺乏对其绝大部分数据截止后（2021 年 9 月）发生的事件的了解，也不会从其经验中学习。它有时会犯一些简单的推理错误，这似乎与这么多领域的能力不相符，或者过于轻信用户的明显虚假陈述。有时它也会像人类一样在困难的问题上失败，比如在它生成的代码中引入安全漏洞。</p><p>GPT-4 预测时也可能出错但很自信，意识到可能出错时也不会 double-check。有趣的是，基础预训练模型经过高度校准（其对答案的预测置信度通常与正确概率相匹配）。然而，通过 OpenAI 目前的后训练（post-training）过程，校准减少了。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/75DBF9C5CF8352FA7583E64C74695DDAAAA470A6_size278_w1080_h612.png" src="https://x.liuping.win/img/32f9f36c35bf34b1e7839ce8053d4de1.png"/><br/></p><p><strong>风险及缓解措施</strong></p><p>OpenAI 表示，研究团队一直在对 GPT-4 进行迭代，使其从训练开始就更加安全和一致，所做的努力包括预训练数据的选择和过滤、评估和专家参与、模型安全改进以及监测和执行。</p><p>GPT-4 有着与以前的模型类似的风险，如产生有害的建议、错误的代码或不准确的信息。同时，GPT-4 的额外能力导致了新的风险面。为了了解这些风险的程度，团队聘请了 50 多位来自 人工智能对齐风险、网络安全、生物风险、信任和安全以及国际安全等领域的专家，对该模型在高风险领域的行为进行对抗性测试。这些领域需要专业知识来评估，来自这些专家的反馈和数据为缓解措施和模型的改进提供了依据。</p><p><strong>预防风险</strong></p><p>按照 demo 视频里 OpenAI 工程师们的说法，GPT-4 的训练在去年 8 月完成，剩下的时间都在进行微调提升，以及最重要的去除危险内容生成的工作。</p><p>GPT-4 在 RLHF 训练中加入了一个额外的安全奖励信号，通过训练模型拒绝对此类内容的请求来减少有害的输出。奖励是由 GPT-4 的零样本分类器提供的，它判断安全边界和安全相关 prompt 的完成方式。为了防止模型拒绝有效的请求，团队从各种来源（例如，标注的生产数据、人类的红队、模型生成的 prompt）收集多样化的数据集，在允许和不允许的类别上应用安全奖励信号（有正值或负值）。</p><p>这些措施大大在许多方面改善了 GPT-4 的安全性能。与 GPT-3.5 相比，模型对不允许内容的请求的响应倾向降低了 82%，而 GPT-4 对敏感请求（如医疗建议和自我伤害）的响应符合政策的频率提高了 29%。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/01DB5F3C7C0086C4C846B03C46735FC6223D8E36_size108_w1080_h605.png" src="https://x.liuping.win/img/128291356c07c1f3ffe2cdace9cc9e49.png"/><br/></p><p><strong>训练过程</strong></p><p>与之前的 GPT 模型一样，GPT-4 基础模型经过训练可以预测文档中的下一个单词。OpenAI 使用公开可用的数据（例如互联网数据）以及已获得许可的数据进行训练。训练数据是一个网络规模的数据 语料库，包括数学问题的正确和错误解决方案、弱推理和强推理、自相矛盾和一致的陈述，以及各种各样的意识形态和想法。</p><p>因此，当提出问题时，基础模型的回应可能与用户的意图相去甚远。为了使其与用户意图保持一致，OpenAI 依然使用 强化学习人类反馈 (RLHF) 来微调模型的行为。请注意，该模型的能力似乎主要来自预训练过程 ——RLHF 不会提高考试成绩（甚至可能会降低它）。但是模型的控制来自后训练过程 —— 基础模型甚至需要及时的工程设计来回答问题。</p><p>GPT-4 的一大重点是建立了一个可预测扩展的 深度学习栈。主要原因是，对于像 GPT-4 这样的大型训练，进行广泛的特定模型调整是不可行的。团队开发了基础设施和优化，在多种规模下都有可预测的行为。为了验证这种可扩展性，他们提前准确地预测了 GPT-4 在内部代码库（不属于训练集）上的最终损失，方法是通过使用相同的方法训练的模型进行推断，但使用的计算量为 1/10000。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/8FD87F6D89168529AD576943FB6CA9CBD01B0C24_size80_w1080_h591.png" src="https://x.liuping.win/img/0d7149052e49fce319f8c2f77fef1048.png"/><br/></p><p>现在，OpenAI 可以准确地预测在训练过程中优化的指标（损失）。例如从计算量为 1/1000 的模型中推断并成功地预测了 HumanEval 数据集的一个子集的通过率：</p><p><img lazyload="https://x0.ifengimg.com/res/2023/EF1097F1D06CB2DE985363F7D4CF66CD5495503E_size81_w1080_h574.png" src="https://x.liuping.win/img/043c12172b354a1ac2e659ca15eb8537.png"/><br/></p><p>有些能力仍然难以预测。例如，Inverse Scaling 竞赛旨在找到一个随着模型计算量的增加而变得更糟的指标，而 hindsight neglect 任务是获胜者之一。GPT-4 扭转了这一趋势。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/C4041A7AD5EE47FD956887D87C3A0D1BFA6FF9B8_size74_w1080_h587.png" src="https://x.liuping.win/img/721683a461a9551d8160c6736c30981b.png"/><br/></p><p>能够准确预测未来的 机器学习能力对于技术安全来说至关重要，但它并没有得到足够的重视，OpenAI 表示正在投入更多精力开发相关方法，并呼吁业界共同努力。</p><p>OpenAI 表示正在开源 OpenAI Evals 软件框架，它被用于创建和运行 基准测试以评估 GPT-4 等模型，同时可以逐样本地检查模型性能。</p><p><strong>ChatGPT 直接升级至 GPT-4 版</strong></p><p>GPT-4 发布后，OpenAI 直接升级了 ChatGPT。ChatGPT Plus 订阅者可以在 chat.openai.com 上获得具有使用上限的 GPT-4 访问权限。</p><p>要访问 GPT-4 API（它使用与 gpt-3.5-turbo 相同的 ChatCompletions API），用户可以注册等待。OpenAI 会邀请部分开发者体验。</p><p>获得访问权限后，用户目前可以向 GPT-4 模型发出纯文本请求（图像输入仍处于有限的 alpha 阶段）。至于价格方面，定价为每 1k 个 prompt token 0.03 美元，每 1k 个 completion token 0.06 美元。默认速率限制为每分钟 40k 个 token 和每分钟 200 个请求。</p><p>GPT-4 的上下文长度为 8,192 个 token。OpenAI 还提供了 32,768 个 token 上下文（约 50 页文本）版本的有限访问，该版本也将随着时间自动更新（当前版本 gpt-4-32k-0314，也支持到 6 月 14 日)。定价为每 1K prompt token 0.06 美元和每 1k completion token 0.12 美元。</p><p>以上，就是今天 OpenAI 关于 GPT-4 的所有内容了。令人不满的一点是，OpenAI 公开的技术报告中，不包含任何关于模型架构、硬件、算力等方面的更多信息，可以说是很不 Open 了。</p><p>不管怎样，迫不及待的用户大概已经开始测试体验了吧。</p><p><img lazyload="https://x0.ifengimg.com/res/2023/03FDD50817D5E49547A5F3FB780C435D63AE24E2_size21_w1080_h574.png" src="https://x.liuping.win/img/e12b582d454959b5709b66566b7a5bb5.png"/><br/></p> </div></body>
        </html>