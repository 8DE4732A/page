<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="ChatGPT最近火得一塌糊涂，也将AI人工智能推向了新的高潮，而生成式AI需要海量的数据进行推理训练，高算力的GPU加速卡自然成了抢手货。比如ChatGPT背后的GPT模型，其训练参数在2018年约为1.2亿个，到了2020年就猛增到近1800亿个，如今恐怕至少在万亿级别。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/9e9c026b94136131e69c9b0fe437a2c8.jpg">
        <meta property="og:url" content="https://x.liuping.win/artical/710be7ea9d7cfc9debb50dbd66aceaff.htm">
        <meta property="og:title" content="NVIDIA找到新摇钱树：加速卡靠ChatGPT轻松稳赚50亿 - x.liuping.win">
        <meta name="keywords" content="nVIDIA,NVIDIA找到新摇钱树：加速卡靠ChatGPT轻松稳赚50亿,x.liuping.win">
        <meta name="description" content="ChatGPT最近火得一塌糊涂，也将AI人工智能推向了新的高潮，而生成式AI需要海量的数据进行推理训练，高算力的GPU加速卡自然成了抢手货。比如ChatGPT背后的GPT模型，其训练参数在2018年约为1.2亿个，到了2020年就猛增到近1800亿个，如今恐怕至少在万亿级别。">
        <title>NVIDIA找到新摇钱树：加速卡靠ChatGPT轻松稳赚50亿</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>NVIDIA找到新摇钱树：加速卡靠ChatGPT轻松稳赚50亿</h1><p>ChatGPT最近火得一塌糊涂，也将AI人工智能推向了新的高潮，而生成式AI需要海量的数据进行推理训练，高算力的GPU加速卡自然成了抢手货。比如ChatGPT背后的GPT模型，其训练参数在2018年约为1.2亿个，到了2020年就猛增到近1800亿个，如今恐怕至少在万亿级别。</p><hr><div class="article-content" id="artibody">
<p><strong>集邦咨询估计，生成式AI对于GPU计算卡的需求量在2万块左右，未来将达到3万块。</strong></p><p>这其中受益最多的，自然是NVIDIA，其旗舰级计算卡A100已经在为ChatGPT默默服务，有估算需要3万块才能满足ChatGPT当前的访问量。</p><p><strong>A100计算卡单块售价8-9万元左右，3万块就是大约25个亿。</strong></p><p><img src="https://x.liuping.win/img/9e9c026b94136131e69c9b0fe437a2c8.jpg"/></p><p>但它往往不是单卖的，而是以DGX A100服务器整套方案的方式销售，每套八块计算卡，算力高达5PFlops(每秒5千万亿次浮点计算)，相当于前几年的一套大型超级计算机，是大规模数据分析、AI加速计算的几乎不二之选。</p><p><strong>DGX A100服务器单套售价约20万美元，3万块卡下来就是大约7.5亿美元，相当于人民币超过50亿元。</strong></p><p>当然，AMD也有MI100、MI200系列加速卡，今年还会推出CPU+GPU融合的全新MI300，算力也很强，但市场份额远不如NVIDIA。</p><p>另外，有数据显示，<strong>GPT-3训练一次的成本约为140万美元</strong>，对于一些更大的LLM模型，训练成本介于200万美元至1200万美元之间。</p><p><img src="https://x.liuping.win/img/f4ce1297233284cc7571c3803cce69f8.jpg"/><br/>DGX A100服务器</p> </div></body>
        </html>