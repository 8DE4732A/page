<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="3月底，马斯克在内的许多社会名流共同签署了一份公开信，希望包括OpenAI在内的机构至少把更高能力的AGI研发暂停6个月，从而让人类社会达成某种基于AGI（通用人工智能）威胁的协议。这封信虽然文本不长，但是涵盖了非常广泛的内容，内含的问题很丰富。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/12be374e1d6b0af4ba700cee08fbad35.jpg">
        <meta property="og:url" content="https://x.liuping.win/artical/4e92bc3a0e15b960600c703a66778965.htm">
        <meta property="og:title" content="为什么全人类都呼吁把通用人工智能“管起来”？ - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,为什么全人类都呼吁把通用人工智能“管起来”？,x.liuping.win">
        <meta name="description" content="3月底，马斯克在内的许多社会名流共同签署了一份公开信，希望包括OpenAI在内的机构至少把更高能力的AGI研发暂停6个月，从而让人类社会达成某种基于AGI（通用人工智能）威胁的协议。这封信虽然文本不长，但是涵盖了非常广泛的内容，内含的问题很丰富。">
        <title>为什么全人类都呼吁把通用人工智能“管起来”？</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>为什么全人类都呼吁把通用人工智能“管起来”？</h1><p>3月底，马斯克在内的许多社会名流共同签署了一份公开信，希望包括OpenAI在内的机构至少把更高能力的AGI研发暂停6个月，从而让人类社会达成某种基于AGI（通用人工智能）威胁的协议。这封信虽然文本不长，但是涵盖了非常广泛的内容，内含的问题很丰富。</p><hr><div class="article-content" id="artibody">
<p>比如，人类该如何算这笔账——AGI让信息渠道充斥宣传甚至谎言；如果是人类本来就喜欢和满意的工作，是否应该被AI取代？而这些效率提升，是否值得我们冒着“失去对文明控制”的风险？而人类针对AGI的监管体系又应该如何搭设？</p><p><img src="https://x.liuping.win/img/12be374e1d6b0af4ba700cee08fbad35.jpg"/></p><p>这封信最终的结果是引发了硅谷规模空前的观点撕裂的大讨论。毕竟如果以GPT3.5进入人类社会视野为起点，对于AGI的发展，人类还有太多的盲点没有解决、有太多争议没有达成共识。但当人类还在争吵时，AGI已经开始在全球各地制造麻烦了，虚假信息和信息泄露都像是一个让人不安的庞大黑箱。</p><p>因此，最近一段时间以来，包括欧美日韩在内的各个信息发达国家都开始了对大模型监管的讨论。而在4月11日，国家互联网信息办公室也发布了《生成式人工智能服务管理办法（征求意见稿）》（以下简称《办法》）。</p><p>这份《办法》的重要意义在于，它可以算是人类社会面对AGI监管中最早的官方文件之一。</p><p>而监管视角往往是人们理解AGI中很独特的一个角度：</p><p>不同于科学家视角有很多参数上的复杂解释体系、有诸多复杂的模型框架流派。监管者是社会利益的代言人，也往往是站在社会整体的角度去思考。因此监管者的思路也往往更接近大部分人朴素的直觉。但它同时又代表了核心专家群体的思考，所以它也能部分回应新事物的很多讨论。</p><p>所以这份《办法》其实可以成为很多人普通人去理解AGI的一个窗口，他可以帮助我们更好地理解和参与AGI讨论。因为AGI既不是天使，也不是魔鬼。而祛魅后的AGI也将成为我们公共生活中的重要部分。</p><p><strong>“我是谁”：生成式人工智能</strong></p><p>如果站在硅谷的角度，AGI约等于大模型。它是通过优化的模型、结构语料库和庞大的算力组合而成，带有一点智慧体思想特征的计算机程序。</p><p>《文件》对这个“新物种”的划入到了“生成式人工智能”行业，即“基于算法、模型、规则生成文本、图片、声音、视频、代码等内容的技术。”</p><p>显然这是从AI与社会关系的角度出发。</p><p>毕竟AGI未来的技术特征会改变，模型大小会浮动，但它最终要追求与社会之间互动的内核不会变。而无论一个模型封闭开发多久，其最终目的都是要面向社会做内容输出。这其实是过去辅助决策、检索、安全、支付等垂直AI与AGI很核心的不同。</p><p>而从技术远景来看，任何有广泛用户基础的“生成式AI”，其大概率都会是有AGI底层能力的产品。</p><p>而对于普通人来说，AGI（通用人工智能）这个名词确实也有点太“魅惑”。许多人将AGI与人类智慧相类比，仿佛让人看到了一张“赛博灵魂”，全然忘了AGI只是有一点智慧的苗头，未来的发展其实还有巨大的不确定性。</p><p>所以与其说我们未来是《西部世界》似的“AGI社会”，不如说我们将迎来一个“生成式人工智能”的世界，这可能更贴近我们这一代人的技术现实。</p><p><strong>“我属于谁”：服务提供者</strong></p><p>在AGI伦理讨论中存在一个经典问题：大模型生成的内容，版权到底是属于大模型公司、还是属于辛苦写prompt（提示词）的用户？</p><p>《办法》虽然没有明确规定版权的归属，但在监管上划出了责任义务，可以供更多人参考。根据《办法》：</p><p>“利用生成式人工智能产品提供聊天和文本、图像、声音生成等服务的组织和个人（以下称‘提供者’），包括通过提供可编程接口等方式支持他人自行生成文本、图像、声音等，承担该产品生成内容生产者的责任”。</p><p>如果沿用这个思路，负责人其实既不属于大模型的研发人员、也不属于用户，而是属于连接大模型与用户之间的服务提供者。</p><p>当然，在大部分情况下AGI的开发者和API提供者应该是同一个主体。但随着技术生态的演进，不同层次的主体可能会变得多元化。</p><p>明确中间的服务提供者承担责任，其实也符合“生成式人工智能”的官方定义。同时，这样的权责划分其实也倒逼未来上游产业链必须达成足够好的内容互信。</p><p><strong>AGI的“内容权”：需要标识</strong></p><p>与版权问题类似的争论是：AGI内容是否可以与人类享受的内容“同权”？</p><p>《办法》明确对AGI内容做出了限制，出现在了两个地方：</p><p>·“按照《互联网信息服务深度合成管理规定》对生成的图片、视频等内容进行标识。”</p><p>·“提供者应当根据国家网信部门和有关主管部门的要求，提供可以影响用户信任、选择的必要信息，包括预训练和优化训练数据的来源、规模、类型、质量等描述，人工标注规则，人工标注数据的规模和类型，基础算法和技术体系等。”</p><p>关于AGI内容一直存在争议。尤其在GPT内测的时候，系统有时候表现得像是一个村口玩手机的大爷。它有时会直接给用户一些包含价值判断的观点，却没有足够说服力的信息佐证。</p><p>如果《办法》落地，AGI就会彻底告别“口说无凭”的跑火车式的内容输出，转而衍生出一些搜索似的工具属性。而像过去拿着AI合成作品去美术比赛拿奖的事情，也将彻底变成一段“黑历史”。</p><p>这其实与法律的精神相匹配，AI在内容生成上是绝对的强者，他自然也需要承担更多的举证义务。而多模态的内容形态可能存在巨大的内容风险，自然也需要相应的制约机制。</p><p>相反，如果让AGI内容与人类内容完全同权，可能会对人类自己的内容生态产生难以评估的影响。</p><p><strong>“监管生态”：阿西罗马原则</strong></p><p>在开头针对OpenAI的公开信里有一个小细节，它提出应该有一个审计的生态（ecosystem），而非体系（system）。</p><p>而公开信也提到了阿西罗马AI原则（Asilomar AI Principles），即先进AI对人类的影响会是一个文明级的，因此对它的规划和管理所需要耗费的资源，应该是与其本身耗费的资源相匹配的。</p><p>换言之，如果生成式AI会是一个庞大的体系，那么就不能仅仅靠某个环节、某个主体完全实现监管。</p><p>除了继续强调既有法律的监管地位外，《办法》中也强调了对生成式AI的全流程监管。</p><p>比如语料库（“预训练数据”）必须合法合规、数据标记必须培训“清晰、具体、可操作的标注规则”、使用场景必须符合规范并承担相应责任、内容本身需要被标注、用户使用过程中要有明确的举报反馈通道等等。</p><p>既然AGI会成为庞大的生态底座，那么监管者其实也需要用更多元化的方式去执行监管任务。</p><p>因此尽管大模型有一定的黑箱属性，但通过一整套组合拳，也足以倒推技术黑箱后的合规能力，从而达到“开箱”的目的。</p><p>而为了进一步明确开发者责任，《办法》中还加入了这么一条：</p><p>“对于运行中发现、用户举报的不符合本办法要求的生成内容，除采取内容过滤等措施外，应在3个月内通过模型优化训练等方式防止再次生成。”</p><p>换言之，如果监管方只是想用“内容过滤”的手段逃避进一步的模型优化责任，在《办法》中是违规的——AGI需要一个合规的灵魂而不只是合规的API。</p><p><strong>“驯火”</strong></p><p>有很多人将AGI的发明比作“火”，人类学会了生火、享受火的文明成果，但同时也要花更漫长的时间去“驯火”。</p><p>在前面提到的公开信里，提出一个好的AGI应该满足至少几个标准，准确、安全、可解释、透明、鲁棒、对齐一致、值得信赖和忠诚的（accurate, safe, interpretable, transparent, robust, aligned, trustworthy, and loyal）。</p><p>换言之，AGI绝不能像现在这样，成为一个玄学般炼丹炉式的黑箱。它应该对人类有更高确定性的利好。</p><p>所以公开信提出了更多远景的技术监管设想，比如建立一个庞大算力规模的、专业垂直的监管体系，达成某种底层的代码公约等等。这些提议是否值得落地，还需要更多的行业讨论。但一个相对具有确定性的事情是：</p><p>就像人类在“驯火”的过程中才更加了解“火”一样，人类社会只有不断与AGI互动、博弈的过程中，或许才可以更加了解AGI。</p> </div></body>
        </html>