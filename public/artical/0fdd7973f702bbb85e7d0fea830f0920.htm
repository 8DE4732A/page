<!DOCTYPE html>
        <html><head>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta charset="utf-8">
        <meta property="og:type" content="article">
        <meta property="og:locale" content="zh_CN">
        <meta property="og:description" content="一群知名的人工智能伦理学家对本周要求人工智能发展&quot;暂停&quot;六个月的有争议的信提出了反驳，批评它只关注假设的未来威胁，而真正的危害是归因于今天对技术的滥用。数千人，包括史蒂夫-沃兹尼亚克和埃隆-马斯克等熟悉的科技业界人物在本周早些时候签署了生命未来研究所的公开信，提议应暂停GPT-4等人工智能模型的开发，以避免&quot;我们的文明失去控制&quot;，以及面临其他威胁。">
        <meta property="og:site_name" content="x.liuping.win">
        <meta property="og:image" content="https://x.liuping.win/img/0cdd1439141ebbb0ec6f4425ba61116f.jpg">
        <meta property="og:url" content="https://x.liuping.win/artical/0fdd7973f702bbb85e7d0fea830f0920.htm">
        <meta property="og:title" content="伦理学家回击"AI暂停发展"信件 称其过于戏剧化以至于忽略了现实危害 - x.liuping.win">
        <meta name="keywords" content="AI 人工智能,伦理学家回击&quot;AI暂停发展&quot;信件 称其过于戏剧化以至于忽略了现实危害,x.liuping.win">
        <meta name="description" content="一群知名的人工智能伦理学家对本周要求人工智能发展&amp;quot;暂停&amp;quot;六个月的有争议的信提出了反驳，批评它只关注假设的未来威胁，而真正的危害是归因于今天对技术的滥用。数千人，包括史蒂夫-沃兹尼亚克和埃隆-马斯克等熟悉的科技业界人物在本周早些时候签署了生命未来研究所的公开信，提议应暂停GPT-4等人工智能模型的开发，以避免&amp;quot;我们的文明失去控制&amp;quot;，以及面临其他威胁。">
        <title>伦理学家回击"AI暂停发展"信件 称其过于戏剧化以至于忽略了现实危害</title>
        <style>img {max-width: 90%;} body {text-align: center;}</style>
        </head>
        <body><h1>伦理学家回击"AI暂停发展"信件 称其过于戏剧化以至于忽略了现实危害</h1><p>一群知名的人工智能伦理学家对本周要求人工智能发展"暂停"六个月的有争议的信提出了反驳，批评它只关注假设的未来威胁，而真正的危害是归因于今天对技术的滥用。数千人，包括史蒂夫-沃兹尼亚克和埃隆-马斯克等熟悉的科技业界人物在本周早些时候签署了生命未来研究所的公开信，提议应暂停GPT-4等人工智能模型的开发，以避免"我们的文明失去控制"，以及面临其他威胁。</p><hr><div class="article-content" id="artibody">
<p><img alt="be9c5ff1129cd60fcdec5258c9cf7b629e036f3d-750x250.jpg" src="https://x.liuping.win/img/0cdd1439141ebbb0ec6f4425ba61116f.jpg" title=""/></p><p>Timnit Gebru、Emily M. Bender、Angelina McMillan-Major和Margaret Mitchell都是人工智能和伦理学领域的重要人物，他们因一篇批评人工智能能力的论文而被Google排挤而闻名（除了他们的工作之外）。他们目前正在DAIR研究所工作，这是一个新的研究机构，旨在研究、揭露和预防与人工智能相关的危害。</p><p>但在签名者名单上找不到他们，现在他们发表了一份反对意见书，指出这封信没有涉及到技术造成的现有问题。</p><p>他们写道："这些假设的风险是一种危险的意识形态的焦点，这种意识形态被称为长期主义，忽视了今天部署人工智能系统所造成的实际危害。"他们提到了工人剥削、数据盗窃、支持现有权力结构的合成媒体以及这些权力结构进一步集中在少数人手中。</p><p>选择担心终结者或黑客帝国式的机器人启示录是一个红鲱鱼（红鲱鱼是英文熟语。指以修辞或文学等各种手法转移议题焦点与注意力，是一种政治宣传、公关及戏剧创作的技巧，也是一种逻辑谬误。）。因为在同一时刻，我们有报告称，像Clearview AI这样的公司早已经被警方利用，基本上被用来陷害无辜者。当你看到每个前门都有Ring的门铃摄像头，还可以用假搜查令轻松读取这些数据时，这种担忧早已经成为现实，至于有没有T-1000已经不重要了。</p><p>虽然DAIR的工作人员同意信中的一些目标，如识别合成媒体，但他们强调，现在必须采取行动，对今天的问题，用我们现有的补救措施：</p><p>我们需要的是实施透明化的监管。我们不仅要清楚地知道我们何时遇到了合成媒体，而且还应该要求建立这些系统的组织记录和披露训练数据和模型架构。创造可安全使用的工具的责任应该由构建和部署生成系统的公司来承担，这意味着这些系统的构建者应该对其产品产生的结果负责。</p><p>目前对越来越大的"人工智能实验"的竞赛并不是一条预设的道路，我们唯一的选择是跑多快，而是由利润动机驱动的一系列决定。公司的行动和选择必须由保护人们权益的监管来塑造。</p><p>现在确实是采取行动的时候了：但我们关注的重点不应该是想象中的"强大的数字思想"。相反，我们应该关注那些声称建立数字思想的公司的非常真实和非常现实的剥削行为，这些公司正在迅速集中权力，增加社会不平等现象。</p><p>顺便说一句，这封信呼应了我在昨天西雅图的非洲科技活动中听到的Uncharted Power创始人杰西卡-马修斯的感慨："你不应该害怕人工智能。你应该害怕打造它的人"。(她的解决办法是：加入他们，成为打造它的人。）</p><p>虽然任何大公司都不可能同意按照这封公开信暂停其研究工作，但从它收到的参与度来看，显然人工智能的风险--真实的和假设的--在社会的许多部分都非常令人担忧。但是，如果他们不做，也许就得有人替他们做。</p><p><strong>阅读反对信全文：</strong></p><p>https://www.dair-institute.org/blog/letter-statement-March2023<br/></p> </div></body>
        </html>